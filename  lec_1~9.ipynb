{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMWROzh5u+rB9a255bN0LYv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peter2760/PyTorchZeroToAll/blob/main/%20lec_1~9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nX9bOrdVIFX_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18409779-207e-4224-b4b4-ad4c9013451b"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x_data = [1,2,3]\n",
        "y_data = [2, 4, 6]\n",
        "\n",
        "def forward(x):\n",
        "  return x*w\n",
        "\n",
        "def loss(x, y):\n",
        "  y_pred = forward(x)\n",
        "  return (y_pred - y) * (y_pred - y)\n",
        "\n",
        "for w in np.arange(0.0, 4.1, 0.1):\n",
        "  print(\"w=\", w)\n",
        "  l_sum = 0\n",
        "  for x_val, y_val in zip(x_data, y_data):\n",
        "    y_pred_val = forward(x_val)\n",
        "    l = loss(x_val, y_val)\n",
        "    l_sum += l\n",
        "    print(\"\\t\", x_val, y_val,y_pred_val, l)\n",
        "\n",
        "  print(\"MSE=\", l_sum /3)\n",
        "  print(\"-------------------------------------\")\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "w= 0.0\n",
            "\t 1 2 0.0 4.0\n",
            "\t 2 4 0.0 16.0\n",
            "\t 3 6 0.0 36.0\n",
            "MSE= 18.666666666666668\n",
            "-------------------------------------\n",
            "w= 0.1\n",
            "\t 1 2 0.1 3.61\n",
            "\t 2 4 0.2 14.44\n",
            "\t 3 6 0.30000000000000004 32.49\n",
            "MSE= 16.846666666666668\n",
            "-------------------------------------\n",
            "w= 0.2\n",
            "\t 1 2 0.2 3.24\n",
            "\t 2 4 0.4 12.96\n",
            "\t 3 6 0.6000000000000001 29.160000000000004\n",
            "MSE= 15.120000000000003\n",
            "-------------------------------------\n",
            "w= 0.30000000000000004\n",
            "\t 1 2 0.30000000000000004 2.8899999999999997\n",
            "\t 2 4 0.6000000000000001 11.559999999999999\n",
            "\t 3 6 0.9000000000000001 26.009999999999998\n",
            "MSE= 13.486666666666665\n",
            "-------------------------------------\n",
            "w= 0.4\n",
            "\t 1 2 0.4 2.5600000000000005\n",
            "\t 2 4 0.8 10.240000000000002\n",
            "\t 3 6 1.2000000000000002 23.04\n",
            "MSE= 11.946666666666667\n",
            "-------------------------------------\n",
            "w= 0.5\n",
            "\t 1 2 0.5 2.25\n",
            "\t 2 4 1.0 9.0\n",
            "\t 3 6 1.5 20.25\n",
            "MSE= 10.5\n",
            "-------------------------------------\n",
            "w= 0.6000000000000001\n",
            "\t 1 2 0.6000000000000001 1.9599999999999997\n",
            "\t 2 4 1.2000000000000002 7.839999999999999\n",
            "\t 3 6 1.8000000000000003 17.639999999999993\n",
            "MSE= 9.146666666666663\n",
            "-------------------------------------\n",
            "w= 0.7000000000000001\n",
            "\t 1 2 0.7000000000000001 1.6899999999999995\n",
            "\t 2 4 1.4000000000000001 6.759999999999998\n",
            "\t 3 6 2.1 15.209999999999999\n",
            "MSE= 7.886666666666666\n",
            "-------------------------------------\n",
            "w= 0.8\n",
            "\t 1 2 0.8 1.44\n",
            "\t 2 4 1.6 5.76\n",
            "\t 3 6 2.4000000000000004 12.959999999999997\n",
            "MSE= 6.719999999999999\n",
            "-------------------------------------\n",
            "w= 0.9\n",
            "\t 1 2 0.9 1.2100000000000002\n",
            "\t 2 4 1.8 4.840000000000001\n",
            "\t 3 6 2.7 10.889999999999999\n",
            "MSE= 5.646666666666666\n",
            "-------------------------------------\n",
            "w= 1.0\n",
            "\t 1 2 1.0 1.0\n",
            "\t 2 4 2.0 4.0\n",
            "\t 3 6 3.0 9.0\n",
            "MSE= 4.666666666666667\n",
            "-------------------------------------\n",
            "w= 1.1\n",
            "\t 1 2 1.1 0.8099999999999998\n",
            "\t 2 4 2.2 3.2399999999999993\n",
            "\t 3 6 3.3000000000000003 7.289999999999998\n",
            "MSE= 3.779999999999999\n",
            "-------------------------------------\n",
            "w= 1.2000000000000002\n",
            "\t 1 2 1.2000000000000002 0.6399999999999997\n",
            "\t 2 4 2.4000000000000004 2.5599999999999987\n",
            "\t 3 6 3.6000000000000005 5.759999999999997\n",
            "MSE= 2.986666666666665\n",
            "-------------------------------------\n",
            "w= 1.3\n",
            "\t 1 2 1.3 0.48999999999999994\n",
            "\t 2 4 2.6 1.9599999999999997\n",
            "\t 3 6 3.9000000000000004 4.409999999999998\n",
            "MSE= 2.2866666666666657\n",
            "-------------------------------------\n",
            "w= 1.4000000000000001\n",
            "\t 1 2 1.4000000000000001 0.3599999999999998\n",
            "\t 2 4 2.8000000000000003 1.4399999999999993\n",
            "\t 3 6 4.2 3.2399999999999993\n",
            "MSE= 1.6799999999999995\n",
            "-------------------------------------\n",
            "w= 1.5\n",
            "\t 1 2 1.5 0.25\n",
            "\t 2 4 3.0 1.0\n",
            "\t 3 6 4.5 2.25\n",
            "MSE= 1.1666666666666667\n",
            "-------------------------------------\n",
            "w= 1.6\n",
            "\t 1 2 1.6 0.15999999999999992\n",
            "\t 2 4 3.2 0.6399999999999997\n",
            "\t 3 6 4.800000000000001 1.4399999999999984\n",
            "MSE= 0.746666666666666\n",
            "-------------------------------------\n",
            "w= 1.7000000000000002\n",
            "\t 1 2 1.7000000000000002 0.0899999999999999\n",
            "\t 2 4 3.4000000000000004 0.3599999999999996\n",
            "\t 3 6 5.1000000000000005 0.809999999999999\n",
            "MSE= 0.4199999999999995\n",
            "-------------------------------------\n",
            "w= 1.8\n",
            "\t 1 2 1.8 0.03999999999999998\n",
            "\t 2 4 3.6 0.15999999999999992\n",
            "\t 3 6 5.4 0.3599999999999996\n",
            "MSE= 0.1866666666666665\n",
            "-------------------------------------\n",
            "w= 1.9000000000000001\n",
            "\t 1 2 1.9000000000000001 0.009999999999999974\n",
            "\t 2 4 3.8000000000000003 0.0399999999999999\n",
            "\t 3 6 5.7 0.0899999999999999\n",
            "MSE= 0.046666666666666586\n",
            "-------------------------------------\n",
            "w= 2.0\n",
            "\t 1 2 2.0 0.0\n",
            "\t 2 4 4.0 0.0\n",
            "\t 3 6 6.0 0.0\n",
            "MSE= 0.0\n",
            "-------------------------------------\n",
            "w= 2.1\n",
            "\t 1 2 2.1 0.010000000000000018\n",
            "\t 2 4 4.2 0.04000000000000007\n",
            "\t 3 6 6.300000000000001 0.09000000000000043\n",
            "MSE= 0.046666666666666835\n",
            "-------------------------------------\n",
            "w= 2.2\n",
            "\t 1 2 2.2 0.04000000000000007\n",
            "\t 2 4 4.4 0.16000000000000028\n",
            "\t 3 6 6.6000000000000005 0.36000000000000065\n",
            "MSE= 0.18666666666666698\n",
            "-------------------------------------\n",
            "w= 2.3000000000000003\n",
            "\t 1 2 2.3000000000000003 0.09000000000000016\n",
            "\t 2 4 4.6000000000000005 0.36000000000000065\n",
            "\t 3 6 6.9 0.8100000000000006\n",
            "MSE= 0.42000000000000054\n",
            "-------------------------------------\n",
            "w= 2.4000000000000004\n",
            "\t 1 2 2.4000000000000004 0.16000000000000028\n",
            "\t 2 4 4.800000000000001 0.6400000000000011\n",
            "\t 3 6 7.200000000000001 1.4400000000000026\n",
            "MSE= 0.7466666666666679\n",
            "-------------------------------------\n",
            "w= 2.5\n",
            "\t 1 2 2.5 0.25\n",
            "\t 2 4 5.0 1.0\n",
            "\t 3 6 7.5 2.25\n",
            "MSE= 1.1666666666666667\n",
            "-------------------------------------\n",
            "w= 2.6\n",
            "\t 1 2 2.6 0.3600000000000001\n",
            "\t 2 4 5.2 1.4400000000000004\n",
            "\t 3 6 7.800000000000001 3.2400000000000024\n",
            "MSE= 1.6800000000000008\n",
            "-------------------------------------\n",
            "w= 2.7\n",
            "\t 1 2 2.7 0.49000000000000027\n",
            "\t 2 4 5.4 1.960000000000001\n",
            "\t 3 6 8.100000000000001 4.410000000000006\n",
            "MSE= 2.2866666666666693\n",
            "-------------------------------------\n",
            "w= 2.8000000000000003\n",
            "\t 1 2 2.8000000000000003 0.6400000000000005\n",
            "\t 2 4 5.6000000000000005 2.560000000000002\n",
            "\t 3 6 8.4 5.760000000000002\n",
            "MSE= 2.986666666666668\n",
            "-------------------------------------\n",
            "w= 2.9000000000000004\n",
            "\t 1 2 2.9000000000000004 0.8100000000000006\n",
            "\t 2 4 5.800000000000001 3.2400000000000024\n",
            "\t 3 6 8.700000000000001 7.290000000000005\n",
            "MSE= 3.780000000000003\n",
            "-------------------------------------\n",
            "w= 3.0\n",
            "\t 1 2 3.0 1.0\n",
            "\t 2 4 6.0 4.0\n",
            "\t 3 6 9.0 9.0\n",
            "MSE= 4.666666666666667\n",
            "-------------------------------------\n",
            "w= 3.1\n",
            "\t 1 2 3.1 1.2100000000000002\n",
            "\t 2 4 6.2 4.840000000000001\n",
            "\t 3 6 9.3 10.890000000000004\n",
            "MSE= 5.646666666666668\n",
            "-------------------------------------\n",
            "w= 3.2\n",
            "\t 1 2 3.2 1.4400000000000004\n",
            "\t 2 4 6.4 5.760000000000002\n",
            "\t 3 6 9.600000000000001 12.96000000000001\n",
            "MSE= 6.720000000000003\n",
            "-------------------------------------\n",
            "w= 3.3000000000000003\n",
            "\t 1 2 3.3000000000000003 1.6900000000000006\n",
            "\t 2 4 6.6000000000000005 6.7600000000000025\n",
            "\t 3 6 9.9 15.210000000000003\n",
            "MSE= 7.886666666666668\n",
            "-------------------------------------\n",
            "w= 3.4000000000000004\n",
            "\t 1 2 3.4000000000000004 1.960000000000001\n",
            "\t 2 4 6.800000000000001 7.840000000000004\n",
            "\t 3 6 10.200000000000001 17.640000000000008\n",
            "MSE= 9.14666666666667\n",
            "-------------------------------------\n",
            "w= 3.5\n",
            "\t 1 2 3.5 2.25\n",
            "\t 2 4 7.0 9.0\n",
            "\t 3 6 10.5 20.25\n",
            "MSE= 10.5\n",
            "-------------------------------------\n",
            "w= 3.6\n",
            "\t 1 2 3.6 2.5600000000000005\n",
            "\t 2 4 7.2 10.240000000000002\n",
            "\t 3 6 10.8 23.040000000000006\n",
            "MSE= 11.94666666666667\n",
            "-------------------------------------\n",
            "w= 3.7\n",
            "\t 1 2 3.7 2.8900000000000006\n",
            "\t 2 4 7.4 11.560000000000002\n",
            "\t 3 6 11.100000000000001 26.010000000000016\n",
            "MSE= 13.486666666666673\n",
            "-------------------------------------\n",
            "w= 3.8000000000000003\n",
            "\t 1 2 3.8000000000000003 3.240000000000001\n",
            "\t 2 4 7.6000000000000005 12.960000000000004\n",
            "\t 3 6 11.4 29.160000000000004\n",
            "MSE= 15.120000000000005\n",
            "-------------------------------------\n",
            "w= 3.9000000000000004\n",
            "\t 1 2 3.9000000000000004 3.610000000000001\n",
            "\t 2 4 7.800000000000001 14.440000000000005\n",
            "\t 3 6 11.700000000000001 32.49000000000001\n",
            "MSE= 16.84666666666667\n",
            "-------------------------------------\n",
            "w= 4.0\n",
            "\t 1 2 4.0 4.0\n",
            "\t 2 4 8.0 16.0\n",
            "\t 3 6 12.0 36.0\n",
            "MSE= 18.666666666666668\n",
            "-------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwO40XIQI5Il"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "d_tpeTRGQbM1",
        "outputId": "1612eff6-b0e6-4e41-933b-d771d3f04710"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "w = 1.0\n",
        "\n",
        "def forward(x):\n",
        "  return x*w\n",
        "\n",
        "def loss(x, y):\n",
        "  y_pred = forward(x)\n",
        "  return (y_pred - y) * (y_pred - y)\n",
        "\n",
        "x_data = [1,2,3]\n",
        "y_data = [2, 4, 6]\n",
        "\n",
        "w_list = []\n",
        "mse_list = []\n",
        "for w in np.arange(0.0, 4.1, 0.1):\n",
        "  print(\"w=\", w)\n",
        "  l_sum = 0\n",
        "  for x_val, y_val in zip(x_data, y_data):\n",
        "    y_pred_val = forward(x_val)\n",
        "    l = loss(x_val, y_val)\n",
        "    l_sum =+ l\n",
        "    print(\"\\t\", x_val, y_val, y_pred_val, l)\n",
        "\n",
        "  print(\"MSE=\", l_sum /3)\n",
        "  print(\"---------------------------------------\")\n",
        "  w_list.append(w)\n",
        "  mse_list.append(l_sum/3)\n",
        "\n",
        "plt.plot(w_list, mse_list)\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('w')\n",
        "plt.show()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "w= 0.0\n",
            "\t 1 2 0.0 4.0\n",
            "\t 2 4 0.0 16.0\n",
            "\t 3 6 0.0 36.0\n",
            "MSE= 12.0\n",
            "---------------------------------------\n",
            "w= 0.1\n",
            "\t 1 2 0.1 3.61\n",
            "\t 2 4 0.2 14.44\n",
            "\t 3 6 0.30000000000000004 32.49\n",
            "MSE= 10.83\n",
            "---------------------------------------\n",
            "w= 0.2\n",
            "\t 1 2 0.2 3.24\n",
            "\t 2 4 0.4 12.96\n",
            "\t 3 6 0.6000000000000001 29.160000000000004\n",
            "MSE= 9.72\n",
            "---------------------------------------\n",
            "w= 0.30000000000000004\n",
            "\t 1 2 0.30000000000000004 2.8899999999999997\n",
            "\t 2 4 0.6000000000000001 11.559999999999999\n",
            "\t 3 6 0.9000000000000001 26.009999999999998\n",
            "MSE= 8.67\n",
            "---------------------------------------\n",
            "w= 0.4\n",
            "\t 1 2 0.4 2.5600000000000005\n",
            "\t 2 4 0.8 10.240000000000002\n",
            "\t 3 6 1.2000000000000002 23.04\n",
            "MSE= 7.68\n",
            "---------------------------------------\n",
            "w= 0.5\n",
            "\t 1 2 0.5 2.25\n",
            "\t 2 4 1.0 9.0\n",
            "\t 3 6 1.5 20.25\n",
            "MSE= 6.75\n",
            "---------------------------------------\n",
            "w= 0.6000000000000001\n",
            "\t 1 2 0.6000000000000001 1.9599999999999997\n",
            "\t 2 4 1.2000000000000002 7.839999999999999\n",
            "\t 3 6 1.8000000000000003 17.639999999999993\n",
            "MSE= 5.879999999999998\n",
            "---------------------------------------\n",
            "w= 0.7000000000000001\n",
            "\t 1 2 0.7000000000000001 1.6899999999999995\n",
            "\t 2 4 1.4000000000000001 6.759999999999998\n",
            "\t 3 6 2.1 15.209999999999999\n",
            "MSE= 5.069999999999999\n",
            "---------------------------------------\n",
            "w= 0.8\n",
            "\t 1 2 0.8 1.44\n",
            "\t 2 4 1.6 5.76\n",
            "\t 3 6 2.4000000000000004 12.959999999999997\n",
            "MSE= 4.319999999999999\n",
            "---------------------------------------\n",
            "w= 0.9\n",
            "\t 1 2 0.9 1.2100000000000002\n",
            "\t 2 4 1.8 4.840000000000001\n",
            "\t 3 6 2.7 10.889999999999999\n",
            "MSE= 3.6299999999999994\n",
            "---------------------------------------\n",
            "w= 1.0\n",
            "\t 1 2 1.0 1.0\n",
            "\t 2 4 2.0 4.0\n",
            "\t 3 6 3.0 9.0\n",
            "MSE= 3.0\n",
            "---------------------------------------\n",
            "w= 1.1\n",
            "\t 1 2 1.1 0.8099999999999998\n",
            "\t 2 4 2.2 3.2399999999999993\n",
            "\t 3 6 3.3000000000000003 7.289999999999998\n",
            "MSE= 2.4299999999999993\n",
            "---------------------------------------\n",
            "w= 1.2000000000000002\n",
            "\t 1 2 1.2000000000000002 0.6399999999999997\n",
            "\t 2 4 2.4000000000000004 2.5599999999999987\n",
            "\t 3 6 3.6000000000000005 5.759999999999997\n",
            "MSE= 1.919999999999999\n",
            "---------------------------------------\n",
            "w= 1.3\n",
            "\t 1 2 1.3 0.48999999999999994\n",
            "\t 2 4 2.6 1.9599999999999997\n",
            "\t 3 6 3.9000000000000004 4.409999999999998\n",
            "MSE= 1.4699999999999995\n",
            "---------------------------------------\n",
            "w= 1.4000000000000001\n",
            "\t 1 2 1.4000000000000001 0.3599999999999998\n",
            "\t 2 4 2.8000000000000003 1.4399999999999993\n",
            "\t 3 6 4.2 3.2399999999999993\n",
            "MSE= 1.0799999999999998\n",
            "---------------------------------------\n",
            "w= 1.5\n",
            "\t 1 2 1.5 0.25\n",
            "\t 2 4 3.0 1.0\n",
            "\t 3 6 4.5 2.25\n",
            "MSE= 0.75\n",
            "---------------------------------------\n",
            "w= 1.6\n",
            "\t 1 2 1.6 0.15999999999999992\n",
            "\t 2 4 3.2 0.6399999999999997\n",
            "\t 3 6 4.800000000000001 1.4399999999999984\n",
            "MSE= 0.4799999999999995\n",
            "---------------------------------------\n",
            "w= 1.7000000000000002\n",
            "\t 1 2 1.7000000000000002 0.0899999999999999\n",
            "\t 2 4 3.4000000000000004 0.3599999999999996\n",
            "\t 3 6 5.1000000000000005 0.809999999999999\n",
            "MSE= 0.2699999999999997\n",
            "---------------------------------------\n",
            "w= 1.8\n",
            "\t 1 2 1.8 0.03999999999999998\n",
            "\t 2 4 3.6 0.15999999999999992\n",
            "\t 3 6 5.4 0.3599999999999996\n",
            "MSE= 0.11999999999999987\n",
            "---------------------------------------\n",
            "w= 1.9000000000000001\n",
            "\t 1 2 1.9000000000000001 0.009999999999999974\n",
            "\t 2 4 3.8000000000000003 0.0399999999999999\n",
            "\t 3 6 5.7 0.0899999999999999\n",
            "MSE= 0.029999999999999968\n",
            "---------------------------------------\n",
            "w= 2.0\n",
            "\t 1 2 2.0 0.0\n",
            "\t 2 4 4.0 0.0\n",
            "\t 3 6 6.0 0.0\n",
            "MSE= 0.0\n",
            "---------------------------------------\n",
            "w= 2.1\n",
            "\t 1 2 2.1 0.010000000000000018\n",
            "\t 2 4 4.2 0.04000000000000007\n",
            "\t 3 6 6.300000000000001 0.09000000000000043\n",
            "MSE= 0.03000000000000014\n",
            "---------------------------------------\n",
            "w= 2.2\n",
            "\t 1 2 2.2 0.04000000000000007\n",
            "\t 2 4 4.4 0.16000000000000028\n",
            "\t 3 6 6.6000000000000005 0.36000000000000065\n",
            "MSE= 0.12000000000000022\n",
            "---------------------------------------\n",
            "w= 2.3000000000000003\n",
            "\t 1 2 2.3000000000000003 0.09000000000000016\n",
            "\t 2 4 4.6000000000000005 0.36000000000000065\n",
            "\t 3 6 6.9 0.8100000000000006\n",
            "MSE= 0.2700000000000002\n",
            "---------------------------------------\n",
            "w= 2.4000000000000004\n",
            "\t 1 2 2.4000000000000004 0.16000000000000028\n",
            "\t 2 4 4.800000000000001 0.6400000000000011\n",
            "\t 3 6 7.200000000000001 1.4400000000000026\n",
            "MSE= 0.48000000000000087\n",
            "---------------------------------------\n",
            "w= 2.5\n",
            "\t 1 2 2.5 0.25\n",
            "\t 2 4 5.0 1.0\n",
            "\t 3 6 7.5 2.25\n",
            "MSE= 0.75\n",
            "---------------------------------------\n",
            "w= 2.6\n",
            "\t 1 2 2.6 0.3600000000000001\n",
            "\t 2 4 5.2 1.4400000000000004\n",
            "\t 3 6 7.800000000000001 3.2400000000000024\n",
            "MSE= 1.0800000000000007\n",
            "---------------------------------------\n",
            "w= 2.7\n",
            "\t 1 2 2.7 0.49000000000000027\n",
            "\t 2 4 5.4 1.960000000000001\n",
            "\t 3 6 8.100000000000001 4.410000000000006\n",
            "MSE= 1.4700000000000022\n",
            "---------------------------------------\n",
            "w= 2.8000000000000003\n",
            "\t 1 2 2.8000000000000003 0.6400000000000005\n",
            "\t 2 4 5.6000000000000005 2.560000000000002\n",
            "\t 3 6 8.4 5.760000000000002\n",
            "MSE= 1.9200000000000006\n",
            "---------------------------------------\n",
            "w= 2.9000000000000004\n",
            "\t 1 2 2.9000000000000004 0.8100000000000006\n",
            "\t 2 4 5.800000000000001 3.2400000000000024\n",
            "\t 3 6 8.700000000000001 7.290000000000005\n",
            "MSE= 2.430000000000002\n",
            "---------------------------------------\n",
            "w= 3.0\n",
            "\t 1 2 3.0 1.0\n",
            "\t 2 4 6.0 4.0\n",
            "\t 3 6 9.0 9.0\n",
            "MSE= 3.0\n",
            "---------------------------------------\n",
            "w= 3.1\n",
            "\t 1 2 3.1 1.2100000000000002\n",
            "\t 2 4 6.2 4.840000000000001\n",
            "\t 3 6 9.3 10.890000000000004\n",
            "MSE= 3.6300000000000012\n",
            "---------------------------------------\n",
            "w= 3.2\n",
            "\t 1 2 3.2 1.4400000000000004\n",
            "\t 2 4 6.4 5.760000000000002\n",
            "\t 3 6 9.600000000000001 12.96000000000001\n",
            "MSE= 4.320000000000003\n",
            "---------------------------------------\n",
            "w= 3.3000000000000003\n",
            "\t 1 2 3.3000000000000003 1.6900000000000006\n",
            "\t 2 4 6.6000000000000005 6.7600000000000025\n",
            "\t 3 6 9.9 15.210000000000003\n",
            "MSE= 5.070000000000001\n",
            "---------------------------------------\n",
            "w= 3.4000000000000004\n",
            "\t 1 2 3.4000000000000004 1.960000000000001\n",
            "\t 2 4 6.800000000000001 7.840000000000004\n",
            "\t 3 6 10.200000000000001 17.640000000000008\n",
            "MSE= 5.880000000000003\n",
            "---------------------------------------\n",
            "w= 3.5\n",
            "\t 1 2 3.5 2.25\n",
            "\t 2 4 7.0 9.0\n",
            "\t 3 6 10.5 20.25\n",
            "MSE= 6.75\n",
            "---------------------------------------\n",
            "w= 3.6\n",
            "\t 1 2 3.6 2.5600000000000005\n",
            "\t 2 4 7.2 10.240000000000002\n",
            "\t 3 6 10.8 23.040000000000006\n",
            "MSE= 7.680000000000002\n",
            "---------------------------------------\n",
            "w= 3.7\n",
            "\t 1 2 3.7 2.8900000000000006\n",
            "\t 2 4 7.4 11.560000000000002\n",
            "\t 3 6 11.100000000000001 26.010000000000016\n",
            "MSE= 8.670000000000005\n",
            "---------------------------------------\n",
            "w= 3.8000000000000003\n",
            "\t 1 2 3.8000000000000003 3.240000000000001\n",
            "\t 2 4 7.6000000000000005 12.960000000000004\n",
            "\t 3 6 11.4 29.160000000000004\n",
            "MSE= 9.72\n",
            "---------------------------------------\n",
            "w= 3.9000000000000004\n",
            "\t 1 2 3.9000000000000004 3.610000000000001\n",
            "\t 2 4 7.800000000000001 14.440000000000005\n",
            "\t 3 6 11.700000000000001 32.49000000000001\n",
            "MSE= 10.830000000000004\n",
            "---------------------------------------\n",
            "w= 4.0\n",
            "\t 1 2 4.0 4.0\n",
            "\t 2 4 8.0 16.0\n",
            "\t 3 6 12.0 36.0\n",
            "MSE= 12.0\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ci_s92KbTLRx",
        "outputId": "6ca80280-50ad-4270-9cdb-dabf795cf475"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "w = 1.0\n",
        "\n",
        "def forward(x):\n",
        "  return x*w\n",
        "\n",
        "def loss(x, y):\n",
        "  y_pred = forward(x)\n",
        "  return (y_pred - y) * (y_pred - y)\n",
        "\n",
        "def gradient(x, y):\n",
        "  return 2*x*(x*w - y)\n",
        "\n",
        "x_data = [1,2,3]\n",
        "y_data = [2, 4, 6]\n",
        "\n",
        "print(\"predict (before training)\", 4, forward(4))\n",
        "\n",
        "for epoch in range(100):\n",
        "  for x_val, y_val in zip(x_data, y_data):\n",
        "    grad = gradient(x_val, y_val)\n",
        "    w = w- 0.01* grad\n",
        "    print(\"\\tgrad: \", x_val, y_val, grad)\n",
        "    l = loss(x_val, y_val)\n",
        "\n",
        "  print(\"progress: \", epoch, \"w=\", w, \"loss= \", l)\n",
        "  print(\"---------------------------------------------------\")\n",
        "\n",
        "print(\"predict (after training)\", 4, forward(4))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predict (before training) 4 4.0\n",
            "\tgrad:  1 2 -2.0\n",
            "\tgrad:  2 4 -7.84\n",
            "\tgrad:  3 6 -16.2288\n",
            "progress:  0 w= 1.260688 loss=  4.919240100095999\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -1.478624\n",
            "\tgrad:  2 4 -5.796206079999999\n",
            "\tgrad:  3 6 -11.998146585599997\n",
            "progress:  1 w= 1.453417766656 loss=  2.688769240265834\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -1.093164466688\n",
            "\tgrad:  2 4 -4.285204709416961\n",
            "\tgrad:  3 6 -8.87037374849311\n",
            "progress:  2 w= 1.5959051959019805 loss=  1.4696334962911515\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -0.8081896081960389\n",
            "\tgrad:  2 4 -3.1681032641284723\n",
            "\tgrad:  3 6 -6.557973756745939\n",
            "progress:  3 w= 1.701247862192685 loss=  0.8032755585999681\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -0.59750427561463\n",
            "\tgrad:  2 4 -2.3422167604093502\n",
            "\tgrad:  3 6 -4.848388694047353\n",
            "progress:  4 w= 1.7791289594933983 loss=  0.43905614881022015\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -0.44174208101320334\n",
            "\tgrad:  2 4 -1.7316289575717576\n",
            "\tgrad:  3 6 -3.584471942173538\n",
            "progress:  5 w= 1.836707389300983 loss=  0.2399802903801062\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -0.3265852213980338\n",
            "\tgrad:  2 4 -1.2802140678802925\n",
            "\tgrad:  3 6 -2.650043120512205\n",
            "progress:  6 w= 1.8792758133988885 loss=  0.1311689630744999\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -0.241448373202223\n",
            "\tgrad:  2 4 -0.946477622952715\n",
            "\tgrad:  3 6 -1.9592086795121197\n",
            "progress:  7 w= 1.910747160155559 loss=  0.07169462478267678\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -0.17850567968888198\n",
            "\tgrad:  2 4 -0.6997422643804168\n",
            "\tgrad:  3 6 -1.4484664872674653\n",
            "progress:  8 w= 1.9340143044689266 loss=  0.03918700813247573\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -0.13197139106214673\n",
            "\tgrad:  2 4 -0.5173278529636143\n",
            "\tgrad:  3 6 -1.0708686556346834\n",
            "progress:  9 w= 1.9512159834655312 loss=  0.021418922423117836\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -0.09756803306893769\n",
            "\tgrad:  2 4 -0.38246668963023644\n",
            "\tgrad:  3 6 -0.7917060475345892\n",
            "progress:  10 w= 1.9639333911678687 loss=  0.01170720245384975\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -0.07213321766426262\n",
            "\tgrad:  2 4 -0.2827622132439096\n",
            "\tgrad:  3 6 -0.5853177814148953\n",
            "progress:  11 w= 1.9733355232910992 loss=  0.006398948863435593\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -0.05332895341780164\n",
            "\tgrad:  2 4 -0.2090494973977819\n",
            "\tgrad:  3 6 -0.4327324596134101\n",
            "progress:  12 w= 1.9802866323953892 loss=  0.003497551760830656\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -0.039426735209221686\n",
            "\tgrad:  2 4 -0.15455280202014876\n",
            "\tgrad:  3 6 -0.3199243001817109\n",
            "progress:  13 w= 1.9854256707695 loss=  0.001911699652671057\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -0.02914865846100012\n",
            "\tgrad:  2 4 -0.11426274116712065\n",
            "\tgrad:  3 6 -0.2365238742159388\n",
            "progress:  14 w= 1.9892250235079405 loss=  0.0010449010656399273\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -0.021549952984118992\n",
            "\tgrad:  2 4 -0.08447581569774698\n",
            "\tgrad:  3 6 -0.17486493849433593\n",
            "progress:  15 w= 1.9920339305797026 loss=  0.0005711243580809696\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -0.015932138840594856\n",
            "\tgrad:  2 4 -0.062453984255132156\n",
            "\tgrad:  3 6 -0.12927974740812687\n",
            "progress:  16 w= 1.994110589284741 loss=  0.0003121664271570621\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -0.011778821430517894\n",
            "\tgrad:  2 4 -0.046172980007630926\n",
            "\tgrad:  3 6 -0.09557806861579543\n",
            "progress:  17 w= 1.9956458879852805 loss=  0.0001706246229305199\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -0.008708224029438938\n",
            "\tgrad:  2 4 -0.03413623819540135\n",
            "\tgrad:  3 6 -0.07066201306448505\n",
            "progress:  18 w= 1.9967809527381737 loss=  9.326038746484765e-05\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -0.006438094523652627\n",
            "\tgrad:  2 4 -0.02523733053271826\n",
            "\tgrad:  3 6 -0.052241274202728505\n",
            "progress:  19 w= 1.9976201197307648 loss=  5.097447086306101e-05\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -0.004759760538470381\n",
            "\tgrad:  2 4 -0.01865826131080439\n",
            "\tgrad:  3 6 -0.03862260091336722\n",
            "progress:  20 w= 1.998240525958391 loss=  2.7861740127856012e-05\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -0.0035189480832178432\n",
            "\tgrad:  2 4 -0.01379427648621423\n",
            "\tgrad:  3 6 -0.028554152326460525\n",
            "progress:  21 w= 1.99869919972735 loss=  1.5228732143933469e-05\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -0.002601600545300009\n",
            "\tgrad:  2 4 -0.01019827413757568\n",
            "\tgrad:  3 6 -0.021110427464781978\n",
            "progress:  22 w= 1.9990383027488265 loss=  8.323754426231206e-06\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -0.001923394502346909\n",
            "\tgrad:  2 4 -0.007539706449199102\n",
            "\tgrad:  3 6 -0.01560719234984198\n",
            "progress:  23 w= 1.9992890056818404 loss=  4.549616284094891e-06\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -0.0014219886363191492\n",
            "\tgrad:  2 4 -0.005574195454370212\n",
            "\tgrad:  3 6 -0.011538584590544687\n",
            "progress:  24 w= 1.999474353368653 loss=  2.486739429417538e-06\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -0.0010512932626940419\n",
            "\tgrad:  2 4 -0.004121069589761106\n",
            "\tgrad:  3 6 -0.008530614050808794\n",
            "progress:  25 w= 1.9996113831376856 loss=  1.3592075910762856e-06\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -0.0007772337246287897\n",
            "\tgrad:  2 4 -0.0030467562005451754\n",
            "\tgrad:  3 6 -0.006306785335127074\n",
            "progress:  26 w= 1.9997126908902887 loss=  7.429187207079447e-07\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -0.0005746182194226179\n",
            "\tgrad:  2 4 -0.002252503420136165\n",
            "\tgrad:  3 6 -0.00466268207967957\n",
            "progress:  27 w= 1.9997875889274812 loss=  4.060661735575354e-07\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -0.0004248221450375844\n",
            "\tgrad:  2 4 -0.0016653028085471533\n",
            "\tgrad:  3 6 -0.0034471768136938863\n",
            "progress:  28 w= 1.9998429619451539 loss=  2.2194855602869353e-07\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -0.00031407610969225175\n",
            "\tgrad:  2 4 -0.0012311783499932005\n",
            "\tgrad:  3 6 -0.0025485391844828342\n",
            "progress:  29 w= 1.9998838998815958 loss=  1.213131374411496e-07\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -0.00023220023680847746\n",
            "\tgrad:  2 4 -0.0009102249282886277\n",
            "\tgrad:  3 6 -0.0018841656015560204\n",
            "progress:  30 w= 1.9999141657892625 loss=  6.630760559646474e-08\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -0.00017166842147497974\n",
            "\tgrad:  2 4 -0.0006729402121816719\n",
            "\tgrad:  3 6 -0.0013929862392156878\n",
            "progress:  31 w= 1.9999365417379913 loss=  3.624255915449335e-08\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -0.0001269165240174175\n",
            "\tgrad:  2 4 -0.0004975127741477792\n",
            "\tgrad:  3 6 -0.0010298514424817995\n",
            "progress:  32 w= 1.9999530845453979 loss=  1.9809538924707548e-08\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -9.383090920422887e-05\n",
            "\tgrad:  2 4 -0.00036781716408107457\n",
            "\tgrad:  3 6 -0.0007613815296476645\n",
            "progress:  33 w= 1.9999653148414271 loss=  1.0827542027017377e-08\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -6.937031714571162e-05\n",
            "\tgrad:  2 4 -0.0002719316432120422\n",
            "\tgrad:  3 6 -0.0005628985014531906\n",
            "progress:  34 w= 1.999974356846045 loss=  5.9181421028034105e-09\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -5.1286307909848006e-05\n",
            "\tgrad:  2 4 -0.00020104232700646207\n",
            "\tgrad:  3 6 -0.0004161576169003922\n",
            "progress:  35 w= 1.9999810417085633 loss=  3.2347513278475087e-09\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -3.7916582873442906e-05\n",
            "\tgrad:  2 4 -0.0001486330048638962\n",
            "\tgrad:  3 6 -0.0003076703200690645\n",
            "progress:  36 w= 1.9999859839076413 loss=  1.7680576050779005e-09\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -2.8032184717474706e-05\n",
            "\tgrad:  2 4 -0.0001098861640933535\n",
            "\tgrad:  3 6 -0.00022746435967313516\n",
            "progress:  37 w= 1.9999896377347262 loss=  9.6638887447731e-10\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -2.0724530547688857e-05\n",
            "\tgrad:  2 4 -8.124015974608767e-05\n",
            "\tgrad:  3 6 -0.00016816713067413502\n",
            "progress:  38 w= 1.999992339052936 loss=  5.282109892545845e-10\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -1.5321894128117464e-05\n",
            "\tgrad:  2 4 -6.006182498197177e-05\n",
            "\tgrad:  3 6 -0.00012432797771566584\n",
            "progress:  39 w= 1.9999943361699042 loss=  2.887107421958329e-10\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -1.1327660191629008e-05\n",
            "\tgrad:  2 4 -4.4404427951505454e-05\n",
            "\tgrad:  3 6 -9.191716585732479e-05\n",
            "progress:  40 w= 1.9999958126624442 loss=  1.5780416225633037e-10\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -8.37467511161094e-06\n",
            "\tgrad:  2 4 -3.282872643772805e-05\n",
            "\tgrad:  3 6 -6.795546372551087e-05\n",
            "progress:  41 w= 1.999996904251097 loss=  8.625295142578772e-11\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -6.191497806007362e-06\n",
            "\tgrad:  2 4 -2.4270671399762023e-05\n",
            "\tgrad:  3 6 -5.0240289795056015e-05\n",
            "progress:  42 w= 1.999997711275687 loss=  4.71443308235547e-11\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -4.5774486259198e-06\n",
            "\tgrad:  2 4 -1.794359861406747e-05\n",
            "\tgrad:  3 6 -3.714324913239864e-05\n",
            "progress:  43 w= 1.9999983079186507 loss=  2.5768253628059826e-11\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -3.3841626985164908e-06\n",
            "\tgrad:  2 4 -1.326591777761621e-05\n",
            "\tgrad:  3 6 -2.7460449796734565e-05\n",
            "progress:  44 w= 1.9999987490239537 loss=  1.4084469615916932e-11\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -2.5019520926150562e-06\n",
            "\tgrad:  2 4 -9.807652203264183e-06\n",
            "\tgrad:  3 6 -2.0301840059744336e-05\n",
            "progress:  45 w= 1.9999990751383971 loss=  7.698320862431846e-12\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -1.8497232057157476e-06\n",
            "\tgrad:  2 4 -7.250914967116273e-06\n",
            "\tgrad:  3 6 -1.5009393983689279e-05\n",
            "progress:  46 w= 1.9999993162387186 loss=  4.20776540913866e-12\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -1.3675225627451937e-06\n",
            "\tgrad:  2 4 -5.3606884460322135e-06\n",
            "\tgrad:  3 6 -1.109662508014253e-05\n",
            "progress:  47 w= 1.9999994944870796 loss=  2.299889814334344e-12\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -1.0110258408246864e-06\n",
            "\tgrad:  2 4 -3.963221296032771e-06\n",
            "\tgrad:  3 6 -8.20386808086937e-06\n",
            "progress:  48 w= 1.9999996262682318 loss=  1.2570789110540446e-12\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -7.474635363990956e-07\n",
            "\tgrad:  2 4 -2.930057062755509e-06\n",
            "\tgrad:  3 6 -6.065218119744031e-06\n",
            "progress:  49 w= 1.999999723695619 loss=  6.870969979249939e-13\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -5.526087618612507e-07\n",
            "\tgrad:  2 4 -2.166226346744793e-06\n",
            "\tgrad:  3 6 -4.484088535150477e-06\n",
            "progress:  50 w= 1.9999997957248556 loss=  3.7555501141274804e-13\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -4.08550288710785e-07\n",
            "\tgrad:  2 4 -1.6015171322436572e-06\n",
            "\tgrad:  3 6 -3.3151404608133817e-06\n",
            "progress:  51 w= 1.9999998489769344 loss=  2.052716967104274e-13\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -3.020461312175371e-07\n",
            "\tgrad:  2 4 -1.1840208351543424e-06\n",
            "\tgrad:  3 6 -2.4509231284497446e-06\n",
            "progress:  52 w= 1.9999998883468353 loss=  1.1219786256679713e-13\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -2.2330632942768602e-07\n",
            "\tgrad:  2 4 -8.753608113920563e-07\n",
            "\tgrad:  3 6 -1.811996877876254e-06\n",
            "progress:  53 w= 1.9999999174534755 loss=  6.132535848018759e-14\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -1.6509304900935717e-07\n",
            "\tgrad:  2 4 -6.471647520100987e-07\n",
            "\tgrad:  3 6 -1.3396310407642886e-06\n",
            "progress:  54 w= 1.999999938972364 loss=  3.351935118167793e-14\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -1.220552721115098e-07\n",
            "\tgrad:  2 4 -4.784566662863199e-07\n",
            "\tgrad:  3 6 -9.904052991061008e-07\n",
            "progress:  55 w= 1.9999999548815364 loss=  1.8321081844499955e-14\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -9.023692726373156e-08\n",
            "\tgrad:  2 4 -3.5372875473171916e-07\n",
            "\tgrad:  3 6 -7.322185204827747e-07\n",
            "progress:  56 w= 1.9999999666433785 loss=  1.0013977760018664e-14\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -6.671324292994996e-08\n",
            "\tgrad:  2 4 -2.615159129248923e-07\n",
            "\tgrad:  3 6 -5.413379398078177e-07\n",
            "progress:  57 w= 1.9999999753390494 loss=  5.473462367088053e-15\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -4.932190122985958e-08\n",
            "\tgrad:  2 4 -1.9334185274999527e-07\n",
            "\tgrad:  3 6 -4.002176350326181e-07\n",
            "progress:  58 w= 1.9999999817678633 loss=  2.991697274308627e-15\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -3.6464273378555845e-08\n",
            "\tgrad:  2 4 -1.429399514307761e-07\n",
            "\tgrad:  3 6 -2.9588569994132286e-07\n",
            "progress:  59 w= 1.9999999865207625 loss=  1.6352086111474931e-15\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -2.6958475007887728e-08\n",
            "\tgrad:  2 4 -1.0567722164012139e-07\n",
            "\tgrad:  3 6 -2.1875184863517916e-07\n",
            "progress:  60 w= 1.999999990034638 loss=  8.937759877335403e-16\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -1.993072418216002e-08\n",
            "\tgrad:  2 4 -7.812843882959442e-08\n",
            "\tgrad:  3 6 -1.617258700292723e-07\n",
            "progress:  61 w= 1.9999999926324883 loss=  4.885220495987371e-16\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -1.473502342363986e-08\n",
            "\tgrad:  2 4 -5.7761292637792394e-08\n",
            "\tgrad:  3 6 -1.195658771990793e-07\n",
            "progress:  62 w= 1.99999999455311 loss=  2.670175009618106e-16\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -1.0893780100218464e-08\n",
            "\tgrad:  2 4 -4.270361841918202e-08\n",
            "\tgrad:  3 6 -8.839649012770678e-08\n",
            "progress:  63 w= 1.9999999959730488 loss=  1.4594702493172377e-16\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -8.05390243385773e-09\n",
            "\tgrad:  2 4 -3.1571296688071016e-08\n",
            "\tgrad:  3 6 -6.53525820126788e-08\n",
            "progress:  64 w= 1.9999999970228268 loss=  7.977204100704301e-17\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -5.9543463493128e-09\n",
            "\tgrad:  2 4 -2.334103754719763e-08\n",
            "\tgrad:  3 6 -4.8315948575350376e-08\n",
            "progress:  65 w= 1.9999999977989402 loss=  4.360197735196887e-17\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -4.402119557767037e-09\n",
            "\tgrad:  2 4 -1.725630838222969e-08\n",
            "\tgrad:  3 6 -3.5720557178819945e-08\n",
            "progress:  66 w= 1.9999999983727301 loss=  2.3832065197304227e-17\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -3.254539748809293e-09\n",
            "\tgrad:  2 4 -1.2757796596929438e-08\n",
            "\tgrad:  3 6 -2.6408640607655798e-08\n",
            "progress:  67 w= 1.9999999987969397 loss=  1.3026183953845832e-17\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -2.406120636067044e-09\n",
            "\tgrad:  2 4 -9.431992964437086e-09\n",
            "\tgrad:  3 6 -1.9524227568012975e-08\n",
            "progress:  68 w= 1.999999999110563 loss=  7.11988308874388e-18\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -1.7788739370416806e-09\n",
            "\tgrad:  2 4 -6.97318647269185e-09\n",
            "\tgrad:  3 6 -1.4434496264925656e-08\n",
            "progress:  69 w= 1.9999999993424284 loss=  3.89160224698574e-18\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -1.3151431055291596e-09\n",
            "\tgrad:  2 4 -5.155360582875801e-09\n",
            "\tgrad:  3 6 -1.067159693945996e-08\n",
            "progress:  70 w= 1.9999999995138495 loss=  2.1270797208746147e-18\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -9.72300906454393e-10\n",
            "\tgrad:  2 4 -3.811418736177075e-09\n",
            "\tgrad:  3 6 -7.88963561149103e-09\n",
            "progress:  71 w= 1.9999999996405833 loss=  1.1626238773828175e-18\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -7.18833437218791e-10\n",
            "\tgrad:  2 4 -2.8178277489132597e-09\n",
            "\tgrad:  3 6 -5.832902161273523e-09\n",
            "progress:  72 w= 1.999999999734279 loss=  6.354692062078993e-19\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -5.314420015167798e-10\n",
            "\tgrad:  2 4 -2.0832526814729135e-09\n",
            "\tgrad:  3 6 -4.31233715403323e-09\n",
            "progress:  73 w= 1.9999999998035491 loss=  3.4733644793346653e-19\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -3.92901711165905e-10\n",
            "\tgrad:  2 4 -1.5401742103904326e-09\n",
            "\tgrad:  3 6 -3.188159070077745e-09\n",
            "progress:  74 w= 1.9999999998547615 loss=  1.8984796531526204e-19\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -2.9047697580608656e-10\n",
            "\tgrad:  2 4 -1.1386696030513122e-09\n",
            "\tgrad:  3 6 -2.3570478902001923e-09\n",
            "progress:  75 w= 1.9999999998926234 loss=  1.0376765851119951e-19\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -2.1475310418850313e-10\n",
            "\tgrad:  2 4 -8.418314934033333e-10\n",
            "\tgrad:  3 6 -1.7425900722400911e-09\n",
            "progress:  76 w= 1.9999999999206153 loss=  5.671751114309842e-20\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -1.5876944203796484e-10\n",
            "\tgrad:  2 4 -6.223768167501476e-10\n",
            "\tgrad:  3 6 -1.2883241140571045e-09\n",
            "progress:  77 w= 1.9999999999413098 loss=  3.100089617511693e-20\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -1.17380327679939e-10\n",
            "\tgrad:  2 4 -4.601314884666863e-10\n",
            "\tgrad:  3 6 -9.524754318590567e-10\n",
            "progress:  78 w= 1.9999999999566096 loss=  1.6944600977692705e-20\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -8.678080476443029e-11\n",
            "\tgrad:  2 4 -3.4018121652934497e-10\n",
            "\tgrad:  3 6 -7.041780492045291e-10\n",
            "progress:  79 w= 1.9999999999679208 loss=  9.2616919156479e-21\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -6.415845632545825e-11\n",
            "\tgrad:  2 4 -2.5150193039280566e-10\n",
            "\tgrad:  3 6 -5.206075570640678e-10\n",
            "progress:  80 w= 1.9999999999762834 loss=  5.062350511130293e-21\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -4.743316850408519e-11\n",
            "\tgrad:  2 4 -1.8593837580738182e-10\n",
            "\tgrad:  3 6 -3.8489211817704927e-10\n",
            "progress:  81 w= 1.999999999982466 loss=  2.7669155644059242e-21\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -3.5067948545020045e-11\n",
            "\tgrad:  2 4 -1.3746692673066718e-10\n",
            "\tgrad:  3 6 -2.845563784603655e-10\n",
            "progress:  82 w= 1.9999999999870368 loss=  1.5124150106147723e-21\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -2.5926372160256506e-11\n",
            "\tgrad:  2 4 -1.0163070385260653e-10\n",
            "\tgrad:  3 6 -2.1037571684701106e-10\n",
            "progress:  83 w= 1.999999999990416 loss=  8.26683933105326e-22\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -1.9167778475548403e-11\n",
            "\tgrad:  2 4 -7.51381179497912e-11\n",
            "\tgrad:  3 6 -1.5553425214420713e-10\n",
            "progress:  84 w= 1.9999999999929146 loss=  4.518126871054872e-22\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -1.4170886686315498e-11\n",
            "\tgrad:  2 4 -5.555023108172463e-11\n",
            "\tgrad:  3 6 -1.1499068364173581e-10\n",
            "progress:  85 w= 1.9999999999947617 loss=  2.469467919185614e-22\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -1.0476508549572827e-11\n",
            "\tgrad:  2 4 -4.106759377009439e-11\n",
            "\tgrad:  3 6 -8.500933290633839e-11\n",
            "progress:  86 w= 1.9999999999961273 loss=  1.349840097651456e-22\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -7.745359908994942e-12\n",
            "\tgrad:  2 4 -3.036149109902908e-11\n",
            "\tgrad:  3 6 -6.285105769165966e-11\n",
            "progress:  87 w= 1.999999999997137 loss=  7.376551550022107e-23\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -5.726086271806707e-12\n",
            "\tgrad:  2 4 -2.2446045022661565e-11\n",
            "\tgrad:  3 6 -4.646416584819235e-11\n",
            "progress:  88 w= 1.9999999999978835 loss=  4.031726170507742e-23\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -4.233058348290797e-12\n",
            "\tgrad:  2 4 -1.659294923683774e-11\n",
            "\tgrad:  3 6 -3.4351188560322043e-11\n",
            "progress:  89 w= 1.9999999999984353 loss=  2.2033851437431755e-23\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -3.1294966618133913e-12\n",
            "\tgrad:  2 4 -1.226752033289813e-11\n",
            "\tgrad:  3 6 -2.539835008974478e-11\n",
            "progress:  90 w= 1.9999999999988431 loss=  1.2047849775995315e-23\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -2.3137047833188262e-12\n",
            "\tgrad:  2 4 -9.070078021977679e-12\n",
            "\tgrad:  3 6 -1.8779644506139448e-11\n",
            "progress:  91 w= 1.9999999999991447 loss=  6.5840863393251405e-24\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -1.7106316363424412e-12\n",
            "\tgrad:  2 4 -6.7057470687359455e-12\n",
            "\tgrad:  3 6 -1.3882228699912957e-11\n",
            "progress:  92 w= 1.9999999999993676 loss=  3.5991747246272455e-24\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -1.2647660696529783e-12\n",
            "\tgrad:  2 4 -4.957811938766099e-12\n",
            "\tgrad:  3 6 -1.0263789818054647e-11\n",
            "progress:  93 w= 1.9999999999995324 loss=  1.969312363793734e-24\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -9.352518759442319e-13\n",
            "\tgrad:  2 4 -3.666400516522117e-12\n",
            "\tgrad:  3 6 -7.58859641791787e-12\n",
            "progress:  94 w= 1.9999999999996543 loss=  1.0761829795642296e-24\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -6.914468997365475e-13\n",
            "\tgrad:  2 4 -2.7107205369247822e-12\n",
            "\tgrad:  3 6 -5.611511255665391e-12\n",
            "progress:  95 w= 1.9999999999997444 loss=  5.875191475205477e-25\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -5.111466805374221e-13\n",
            "\tgrad:  2 4 -2.0037305148434825e-12\n",
            "\tgrad:  3 6 -4.1460168631601846e-12\n",
            "progress:  96 w= 1.999999999999811 loss=  3.2110109830478153e-25\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -3.779199175824033e-13\n",
            "\tgrad:  2 4 -1.4814816040598089e-12\n",
            "\tgrad:  3 6 -3.064215547965432e-12\n",
            "progress:  97 w= 1.9999999999998603 loss=  1.757455879087579e-25\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -2.793321129956894e-13\n",
            "\tgrad:  2 4 -1.0942358130705543e-12\n",
            "\tgrad:  3 6 -2.2648549702353193e-12\n",
            "progress:  98 w= 1.9999999999998967 loss=  9.608404711682446e-26\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 -2.0650148258027912e-13\n",
            "\tgrad:  2 4 -8.100187187665142e-13\n",
            "\tgrad:  3 6 -1.6786572132332367e-12\n",
            "progress:  99 w= 1.9999999999999236 loss=  5.250973729513143e-26\n",
            "---------------------------------------------------\n",
            "predict (after training) 4 7.9999999999996945\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69pslWuymvDv",
        "outputId": "d4a4b48b-0012-4a74-8571-3aeedbc97926"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "\n",
        "w = Variable(torch.Tensor([1.0]), requires_grad=True)\n",
        "\n",
        "def forward(x):\n",
        "  return x*w\n",
        "\n",
        "def loss(x, y):\n",
        "  y_pred = forward(x)\n",
        "  return (y_pred - y) * (y_pred - y)\n",
        "\n",
        "#   - w.grad \n",
        "\n",
        "x_data = [1,2,3]\n",
        "y_data = [2, 4, 6]\n",
        "\n",
        "print(\"predict (before training)\", 4, forward(4))\n",
        "\n",
        "for epoch in range(100):\n",
        "  for x_val, y_val in zip(x_data, y_data):\n",
        "    l = loss(x_val, y_val)\n",
        "    l.backward()\n",
        "    print(\"\\tgrad: \", x_val, y_val, w.grad.data[0])\n",
        "    w.data = w.data - 0.01 * w.grad.data\n",
        "\n",
        "    w.grad.data.zero_()\n",
        "\n",
        "  print(\"progress:\", epoch, l.data[0])\n",
        "  print(\"---------------------------------------------------\")\n",
        "\n",
        "print(\"predict (after training)\", 4, forward(4))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predict (before training) 4 tensor([4.], grad_fn=<MulBackward0>)\n",
            "\tgrad:  1 2 tensor(-2.)\n",
            "\tgrad:  2 4 tensor(-7.8400)\n",
            "\tgrad:  3 6 tensor(-16.2288)\n",
            "progress: 0 tensor(7.3159)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-1.4786)\n",
            "\tgrad:  2 4 tensor(-5.7962)\n",
            "\tgrad:  3 6 tensor(-11.9981)\n",
            "progress: 1 tensor(3.9988)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-1.0932)\n",
            "\tgrad:  2 4 tensor(-4.2852)\n",
            "\tgrad:  3 6 tensor(-8.8704)\n",
            "progress: 2 tensor(2.1857)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-0.8082)\n",
            "\tgrad:  2 4 tensor(-3.1681)\n",
            "\tgrad:  3 6 tensor(-6.5580)\n",
            "progress: 3 tensor(1.1946)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-0.5975)\n",
            "\tgrad:  2 4 tensor(-2.3422)\n",
            "\tgrad:  3 6 tensor(-4.8484)\n",
            "progress: 4 tensor(0.6530)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-0.4417)\n",
            "\tgrad:  2 4 tensor(-1.7316)\n",
            "\tgrad:  3 6 tensor(-3.5845)\n",
            "progress: 5 tensor(0.3569)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-0.3266)\n",
            "\tgrad:  2 4 tensor(-1.2802)\n",
            "\tgrad:  3 6 tensor(-2.6500)\n",
            "progress: 6 tensor(0.1951)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-0.2414)\n",
            "\tgrad:  2 4 tensor(-0.9465)\n",
            "\tgrad:  3 6 tensor(-1.9592)\n",
            "progress: 7 tensor(0.1066)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-0.1785)\n",
            "\tgrad:  2 4 tensor(-0.6997)\n",
            "\tgrad:  3 6 tensor(-1.4485)\n",
            "progress: 8 tensor(0.0583)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-0.1320)\n",
            "\tgrad:  2 4 tensor(-0.5173)\n",
            "\tgrad:  3 6 tensor(-1.0709)\n",
            "progress: 9 tensor(0.0319)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-0.0976)\n",
            "\tgrad:  2 4 tensor(-0.3825)\n",
            "\tgrad:  3 6 tensor(-0.7917)\n",
            "progress: 10 tensor(0.0174)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-0.0721)\n",
            "\tgrad:  2 4 tensor(-0.2828)\n",
            "\tgrad:  3 6 tensor(-0.5853)\n",
            "progress: 11 tensor(0.0095)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-0.0533)\n",
            "\tgrad:  2 4 tensor(-0.2090)\n",
            "\tgrad:  3 6 tensor(-0.4327)\n",
            "progress: 12 tensor(0.0052)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-0.0394)\n",
            "\tgrad:  2 4 tensor(-0.1546)\n",
            "\tgrad:  3 6 tensor(-0.3199)\n",
            "progress: 13 tensor(0.0028)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-0.0291)\n",
            "\tgrad:  2 4 tensor(-0.1143)\n",
            "\tgrad:  3 6 tensor(-0.2365)\n",
            "progress: 14 tensor(0.0016)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-0.0215)\n",
            "\tgrad:  2 4 tensor(-0.0845)\n",
            "\tgrad:  3 6 tensor(-0.1749)\n",
            "progress: 15 tensor(0.0008)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-0.0159)\n",
            "\tgrad:  2 4 tensor(-0.0625)\n",
            "\tgrad:  3 6 tensor(-0.1293)\n",
            "progress: 16 tensor(0.0005)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-0.0118)\n",
            "\tgrad:  2 4 tensor(-0.0462)\n",
            "\tgrad:  3 6 tensor(-0.0956)\n",
            "progress: 17 tensor(0.0003)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-0.0087)\n",
            "\tgrad:  2 4 tensor(-0.0341)\n",
            "\tgrad:  3 6 tensor(-0.0707)\n",
            "progress: 18 tensor(0.0001)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-0.0064)\n",
            "\tgrad:  2 4 tensor(-0.0252)\n",
            "\tgrad:  3 6 tensor(-0.0522)\n",
            "progress: 19 tensor(7.5804e-05)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-0.0048)\n",
            "\tgrad:  2 4 tensor(-0.0187)\n",
            "\tgrad:  3 6 tensor(-0.0386)\n",
            "progress: 20 tensor(4.1433e-05)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-0.0035)\n",
            "\tgrad:  2 4 tensor(-0.0138)\n",
            "\tgrad:  3 6 tensor(-0.0286)\n",
            "progress: 21 tensor(2.2647e-05)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-0.0026)\n",
            "\tgrad:  2 4 tensor(-0.0102)\n",
            "\tgrad:  3 6 tensor(-0.0211)\n",
            "progress: 22 tensor(1.2377e-05)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-0.0019)\n",
            "\tgrad:  2 4 tensor(-0.0075)\n",
            "\tgrad:  3 6 tensor(-0.0156)\n",
            "progress: 23 tensor(6.7684e-06)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-0.0014)\n",
            "\tgrad:  2 4 tensor(-0.0056)\n",
            "\tgrad:  3 6 tensor(-0.0115)\n",
            "progress: 24 tensor(3.7001e-06)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-0.0011)\n",
            "\tgrad:  2 4 tensor(-0.0041)\n",
            "\tgrad:  3 6 tensor(-0.0085)\n",
            "progress: 25 tensor(2.0219e-06)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-0.0008)\n",
            "\tgrad:  2 4 tensor(-0.0030)\n",
            "\tgrad:  3 6 tensor(-0.0063)\n",
            "progress: 26 tensor(1.1045e-06)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-0.0006)\n",
            "\tgrad:  2 4 tensor(-0.0023)\n",
            "\tgrad:  3 6 tensor(-0.0047)\n",
            "progress: 27 tensor(6.0411e-07)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-0.0004)\n",
            "\tgrad:  2 4 tensor(-0.0017)\n",
            "\tgrad:  3 6 tensor(-0.0034)\n",
            "progress: 28 tensor(3.2960e-07)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-0.0003)\n",
            "\tgrad:  2 4 tensor(-0.0012)\n",
            "\tgrad:  3 6 tensor(-0.0025)\n",
            "progress: 29 tensor(1.8051e-07)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-0.0002)\n",
            "\tgrad:  2 4 tensor(-0.0009)\n",
            "\tgrad:  3 6 tensor(-0.0019)\n",
            "progress: 30 tensor(9.8744e-08)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-0.0002)\n",
            "\tgrad:  2 4 tensor(-0.0007)\n",
            "\tgrad:  3 6 tensor(-0.0014)\n",
            "progress: 31 tensor(5.4148e-08)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-0.0001)\n",
            "\tgrad:  2 4 tensor(-0.0005)\n",
            "\tgrad:  3 6 tensor(-0.0010)\n",
            "progress: 32 tensor(2.9468e-08)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-9.3937e-05)\n",
            "\tgrad:  2 4 tensor(-0.0004)\n",
            "\tgrad:  3 6 tensor(-0.0008)\n",
            "progress: 33 tensor(1.6088e-08)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-6.9380e-05)\n",
            "\tgrad:  2 4 tensor(-0.0003)\n",
            "\tgrad:  3 6 tensor(-0.0006)\n",
            "progress: 34 tensor(8.7348e-09)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-5.1260e-05)\n",
            "\tgrad:  2 4 tensor(-0.0002)\n",
            "\tgrad:  3 6 tensor(-0.0004)\n",
            "progress: 35 tensor(4.8467e-09)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-3.7909e-05)\n",
            "\tgrad:  2 4 tensor(-0.0001)\n",
            "\tgrad:  3 6 tensor(-0.0003)\n",
            "progress: 36 tensor(2.6521e-09)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-2.8133e-05)\n",
            "\tgrad:  2 4 tensor(-0.0001)\n",
            "\tgrad:  3 6 tensor(-0.0002)\n",
            "progress: 37 tensor(1.4552e-09)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-2.0981e-05)\n",
            "\tgrad:  2 4 tensor(-8.2016e-05)\n",
            "\tgrad:  3 6 tensor(-0.0002)\n",
            "progress: 38 tensor(7.9149e-10)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-1.5497e-05)\n",
            "\tgrad:  2 4 tensor(-6.1035e-05)\n",
            "\tgrad:  3 6 tensor(-0.0001)\n",
            "progress: 39 tensor(4.4020e-10)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-1.1444e-05)\n",
            "\tgrad:  2 4 tensor(-4.4823e-05)\n",
            "\tgrad:  3 6 tensor(-9.1553e-05)\n",
            "progress: 40 tensor(2.3283e-10)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-8.3447e-06)\n",
            "\tgrad:  2 4 tensor(-3.2425e-05)\n",
            "\tgrad:  3 6 tensor(-6.5804e-05)\n",
            "progress: 41 tensor(1.2028e-10)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-5.9605e-06)\n",
            "\tgrad:  2 4 tensor(-2.2888e-05)\n",
            "\tgrad:  3 6 tensor(-4.5776e-05)\n",
            "progress: 42 tensor(5.8208e-11)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-4.2915e-06)\n",
            "\tgrad:  2 4 tensor(-1.7166e-05)\n",
            "\tgrad:  3 6 tensor(-3.7193e-05)\n",
            "progress: 43 tensor(3.8426e-11)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-3.3379e-06)\n",
            "\tgrad:  2 4 tensor(-1.3351e-05)\n",
            "\tgrad:  3 6 tensor(-2.8610e-05)\n",
            "progress: 44 tensor(2.2737e-11)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-2.6226e-06)\n",
            "\tgrad:  2 4 tensor(-1.0490e-05)\n",
            "\tgrad:  3 6 tensor(-2.2888e-05)\n",
            "progress: 45 tensor(1.4552e-11)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-1.9073e-06)\n",
            "\tgrad:  2 4 tensor(-7.6294e-06)\n",
            "\tgrad:  3 6 tensor(-1.4305e-05)\n",
            "progress: 46 tensor(5.6843e-12)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-1.4305e-06)\n",
            "\tgrad:  2 4 tensor(-5.7220e-06)\n",
            "\tgrad:  3 6 tensor(-1.1444e-05)\n",
            "progress: 47 tensor(3.6380e-12)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-1.1921e-06)\n",
            "\tgrad:  2 4 tensor(-4.7684e-06)\n",
            "\tgrad:  3 6 tensor(-1.1444e-05)\n",
            "progress: 48 tensor(3.6380e-12)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-9.5367e-07)\n",
            "\tgrad:  2 4 tensor(-3.8147e-06)\n",
            "\tgrad:  3 6 tensor(-8.5831e-06)\n",
            "progress: 49 tensor(2.0464e-12)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-7.1526e-07)\n",
            "\tgrad:  2 4 tensor(-2.8610e-06)\n",
            "\tgrad:  3 6 tensor(-5.7220e-06)\n",
            "progress: 50 tensor(9.0949e-13)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-7.1526e-07)\n",
            "\tgrad:  2 4 tensor(-2.8610e-06)\n",
            "\tgrad:  3 6 tensor(-5.7220e-06)\n",
            "progress: 51 tensor(9.0949e-13)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-7.1526e-07)\n",
            "\tgrad:  2 4 tensor(-2.8610e-06)\n",
            "\tgrad:  3 6 tensor(-5.7220e-06)\n",
            "progress: 52 tensor(9.0949e-13)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-7.1526e-07)\n",
            "\tgrad:  2 4 tensor(-2.8610e-06)\n",
            "\tgrad:  3 6 tensor(-5.7220e-06)\n",
            "progress: 53 tensor(9.0949e-13)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-7.1526e-07)\n",
            "\tgrad:  2 4 tensor(-2.8610e-06)\n",
            "\tgrad:  3 6 tensor(-5.7220e-06)\n",
            "progress: 54 tensor(9.0949e-13)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-7.1526e-07)\n",
            "\tgrad:  2 4 tensor(-2.8610e-06)\n",
            "\tgrad:  3 6 tensor(-5.7220e-06)\n",
            "progress: 55 tensor(9.0949e-13)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-7.1526e-07)\n",
            "\tgrad:  2 4 tensor(-2.8610e-06)\n",
            "\tgrad:  3 6 tensor(-5.7220e-06)\n",
            "progress: 56 tensor(9.0949e-13)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-7.1526e-07)\n",
            "\tgrad:  2 4 tensor(-2.8610e-06)\n",
            "\tgrad:  3 6 tensor(-5.7220e-06)\n",
            "progress: 57 tensor(9.0949e-13)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-7.1526e-07)\n",
            "\tgrad:  2 4 tensor(-2.8610e-06)\n",
            "\tgrad:  3 6 tensor(-5.7220e-06)\n",
            "progress: 58 tensor(9.0949e-13)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-7.1526e-07)\n",
            "\tgrad:  2 4 tensor(-2.8610e-06)\n",
            "\tgrad:  3 6 tensor(-5.7220e-06)\n",
            "progress: 59 tensor(9.0949e-13)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-7.1526e-07)\n",
            "\tgrad:  2 4 tensor(-2.8610e-06)\n",
            "\tgrad:  3 6 tensor(-5.7220e-06)\n",
            "progress: 60 tensor(9.0949e-13)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-7.1526e-07)\n",
            "\tgrad:  2 4 tensor(-2.8610e-06)\n",
            "\tgrad:  3 6 tensor(-5.7220e-06)\n",
            "progress: 61 tensor(9.0949e-13)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-7.1526e-07)\n",
            "\tgrad:  2 4 tensor(-2.8610e-06)\n",
            "\tgrad:  3 6 tensor(-5.7220e-06)\n",
            "progress: 62 tensor(9.0949e-13)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-7.1526e-07)\n",
            "\tgrad:  2 4 tensor(-2.8610e-06)\n",
            "\tgrad:  3 6 tensor(-5.7220e-06)\n",
            "progress: 63 tensor(9.0949e-13)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-7.1526e-07)\n",
            "\tgrad:  2 4 tensor(-2.8610e-06)\n",
            "\tgrad:  3 6 tensor(-5.7220e-06)\n",
            "progress: 64 tensor(9.0949e-13)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-7.1526e-07)\n",
            "\tgrad:  2 4 tensor(-2.8610e-06)\n",
            "\tgrad:  3 6 tensor(-5.7220e-06)\n",
            "progress: 65 tensor(9.0949e-13)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-7.1526e-07)\n",
            "\tgrad:  2 4 tensor(-2.8610e-06)\n",
            "\tgrad:  3 6 tensor(-5.7220e-06)\n",
            "progress: 66 tensor(9.0949e-13)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-7.1526e-07)\n",
            "\tgrad:  2 4 tensor(-2.8610e-06)\n",
            "\tgrad:  3 6 tensor(-5.7220e-06)\n",
            "progress: 67 tensor(9.0949e-13)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-7.1526e-07)\n",
            "\tgrad:  2 4 tensor(-2.8610e-06)\n",
            "\tgrad:  3 6 tensor(-5.7220e-06)\n",
            "progress: 68 tensor(9.0949e-13)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-7.1526e-07)\n",
            "\tgrad:  2 4 tensor(-2.8610e-06)\n",
            "\tgrad:  3 6 tensor(-5.7220e-06)\n",
            "progress: 69 tensor(9.0949e-13)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-7.1526e-07)\n",
            "\tgrad:  2 4 tensor(-2.8610e-06)\n",
            "\tgrad:  3 6 tensor(-5.7220e-06)\n",
            "progress: 70 tensor(9.0949e-13)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-7.1526e-07)\n",
            "\tgrad:  2 4 tensor(-2.8610e-06)\n",
            "\tgrad:  3 6 tensor(-5.7220e-06)\n",
            "progress: 71 tensor(9.0949e-13)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-7.1526e-07)\n",
            "\tgrad:  2 4 tensor(-2.8610e-06)\n",
            "\tgrad:  3 6 tensor(-5.7220e-06)\n",
            "progress: 72 tensor(9.0949e-13)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-7.1526e-07)\n",
            "\tgrad:  2 4 tensor(-2.8610e-06)\n",
            "\tgrad:  3 6 tensor(-5.7220e-06)\n",
            "progress: 73 tensor(9.0949e-13)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-7.1526e-07)\n",
            "\tgrad:  2 4 tensor(-2.8610e-06)\n",
            "\tgrad:  3 6 tensor(-5.7220e-06)\n",
            "progress: 74 tensor(9.0949e-13)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-7.1526e-07)\n",
            "\tgrad:  2 4 tensor(-2.8610e-06)\n",
            "\tgrad:  3 6 tensor(-5.7220e-06)\n",
            "progress: 75 tensor(9.0949e-13)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-7.1526e-07)\n",
            "\tgrad:  2 4 tensor(-2.8610e-06)\n",
            "\tgrad:  3 6 tensor(-5.7220e-06)\n",
            "progress: 76 tensor(9.0949e-13)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-7.1526e-07)\n",
            "\tgrad:  2 4 tensor(-2.8610e-06)\n",
            "\tgrad:  3 6 tensor(-5.7220e-06)\n",
            "progress: 77 tensor(9.0949e-13)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-7.1526e-07)\n",
            "\tgrad:  2 4 tensor(-2.8610e-06)\n",
            "\tgrad:  3 6 tensor(-5.7220e-06)\n",
            "progress: 78 tensor(9.0949e-13)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-7.1526e-07)\n",
            "\tgrad:  2 4 tensor(-2.8610e-06)\n",
            "\tgrad:  3 6 tensor(-5.7220e-06)\n",
            "progress: 79 tensor(9.0949e-13)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-7.1526e-07)\n",
            "\tgrad:  2 4 tensor(-2.8610e-06)\n",
            "\tgrad:  3 6 tensor(-5.7220e-06)\n",
            "progress: 80 tensor(9.0949e-13)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-7.1526e-07)\n",
            "\tgrad:  2 4 tensor(-2.8610e-06)\n",
            "\tgrad:  3 6 tensor(-5.7220e-06)\n",
            "progress: 81 tensor(9.0949e-13)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-7.1526e-07)\n",
            "\tgrad:  2 4 tensor(-2.8610e-06)\n",
            "\tgrad:  3 6 tensor(-5.7220e-06)\n",
            "progress: 82 tensor(9.0949e-13)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-7.1526e-07)\n",
            "\tgrad:  2 4 tensor(-2.8610e-06)\n",
            "\tgrad:  3 6 tensor(-5.7220e-06)\n",
            "progress: 83 tensor(9.0949e-13)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-7.1526e-07)\n",
            "\tgrad:  2 4 tensor(-2.8610e-06)\n",
            "\tgrad:  3 6 tensor(-5.7220e-06)\n",
            "progress: 84 tensor(9.0949e-13)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-7.1526e-07)\n",
            "\tgrad:  2 4 tensor(-2.8610e-06)\n",
            "\tgrad:  3 6 tensor(-5.7220e-06)\n",
            "progress: 85 tensor(9.0949e-13)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-7.1526e-07)\n",
            "\tgrad:  2 4 tensor(-2.8610e-06)\n",
            "\tgrad:  3 6 tensor(-5.7220e-06)\n",
            "progress: 86 tensor(9.0949e-13)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-7.1526e-07)\n",
            "\tgrad:  2 4 tensor(-2.8610e-06)\n",
            "\tgrad:  3 6 tensor(-5.7220e-06)\n",
            "progress: 87 tensor(9.0949e-13)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-7.1526e-07)\n",
            "\tgrad:  2 4 tensor(-2.8610e-06)\n",
            "\tgrad:  3 6 tensor(-5.7220e-06)\n",
            "progress: 88 tensor(9.0949e-13)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-7.1526e-07)\n",
            "\tgrad:  2 4 tensor(-2.8610e-06)\n",
            "\tgrad:  3 6 tensor(-5.7220e-06)\n",
            "progress: 89 tensor(9.0949e-13)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-7.1526e-07)\n",
            "\tgrad:  2 4 tensor(-2.8610e-06)\n",
            "\tgrad:  3 6 tensor(-5.7220e-06)\n",
            "progress: 90 tensor(9.0949e-13)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-7.1526e-07)\n",
            "\tgrad:  2 4 tensor(-2.8610e-06)\n",
            "\tgrad:  3 6 tensor(-5.7220e-06)\n",
            "progress: 91 tensor(9.0949e-13)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-7.1526e-07)\n",
            "\tgrad:  2 4 tensor(-2.8610e-06)\n",
            "\tgrad:  3 6 tensor(-5.7220e-06)\n",
            "progress: 92 tensor(9.0949e-13)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-7.1526e-07)\n",
            "\tgrad:  2 4 tensor(-2.8610e-06)\n",
            "\tgrad:  3 6 tensor(-5.7220e-06)\n",
            "progress: 93 tensor(9.0949e-13)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-7.1526e-07)\n",
            "\tgrad:  2 4 tensor(-2.8610e-06)\n",
            "\tgrad:  3 6 tensor(-5.7220e-06)\n",
            "progress: 94 tensor(9.0949e-13)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-7.1526e-07)\n",
            "\tgrad:  2 4 tensor(-2.8610e-06)\n",
            "\tgrad:  3 6 tensor(-5.7220e-06)\n",
            "progress: 95 tensor(9.0949e-13)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-7.1526e-07)\n",
            "\tgrad:  2 4 tensor(-2.8610e-06)\n",
            "\tgrad:  3 6 tensor(-5.7220e-06)\n",
            "progress: 96 tensor(9.0949e-13)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-7.1526e-07)\n",
            "\tgrad:  2 4 tensor(-2.8610e-06)\n",
            "\tgrad:  3 6 tensor(-5.7220e-06)\n",
            "progress: 97 tensor(9.0949e-13)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-7.1526e-07)\n",
            "\tgrad:  2 4 tensor(-2.8610e-06)\n",
            "\tgrad:  3 6 tensor(-5.7220e-06)\n",
            "progress: 98 tensor(9.0949e-13)\n",
            "---------------------------------------------------\n",
            "\tgrad:  1 2 tensor(-7.1526e-07)\n",
            "\tgrad:  2 4 tensor(-2.8610e-06)\n",
            "\tgrad:  3 6 tensor(-5.7220e-06)\n",
            "progress: 99 tensor(9.0949e-13)\n",
            "---------------------------------------------------\n",
            "predict (after training) 4 tensor([8.0000], grad_fn=<MulBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obfrpYd1_Vc6",
        "outputId": "0d7c05e0-b6b5-41b4-eda1-4da313a9ec9e"
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "\n",
        "x_data = Variable(torch.Tensor([[1.0], [2.0], [3.0]]))\n",
        "y_data = Variable(torch.Tensor([[2.0], [4.0], [6.0]]))\n",
        "\n",
        "class Model(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Model, self).__init__()\n",
        "    self.linear=torch.nn.Linear(1,1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    y_pred = self.linear(x)\n",
        "    return y_pred\n",
        "\n",
        "model = Model()\n",
        "\n",
        "criterion = torch.nn.MSELoss(size_average=False)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "for epoch in range(500):\n",
        "  y_pred = model(x_data)\n",
        "\n",
        "  loss = criterion(y_pred, y_data)\n",
        "  print(epoch, loss.item())\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "hour_var = Variable(torch.Tensor([[4.0]]))\n",
        "print(\"predict (after training)\", 4, model.forward(hour_var).data[0][0])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0 86.07594299316406\n",
            "1 38.71782684326172\n",
            "2 17.629596710205078\n",
            "3 8.236061096191406\n",
            "4 4.048752307891846\n",
            "5 2.1791865825653076\n",
            "6 1.3414959907531738\n",
            "7 0.9632406234741211\n",
            "8 0.7895921468734741\n",
            "9 0.7071040868759155\n",
            "10 0.6652718782424927\n",
            "11 0.6416120529174805\n",
            "12 0.6261148452758789\n",
            "13 0.614322304725647\n",
            "14 0.6042500734329224\n",
            "15 0.5950124263763428\n",
            "16 0.5862146019935608\n",
            "17 0.5776799321174622\n",
            "18 0.5693288445472717\n",
            "19 0.5611250400543213\n",
            "20 0.5530511140823364\n",
            "21 0.5450987815856934\n",
            "22 0.5372627377510071\n",
            "23 0.5295405983924866\n",
            "24 0.521929919719696\n",
            "25 0.514428973197937\n",
            "26 0.5070353746414185\n",
            "27 0.4997483491897583\n",
            "28 0.49256646633148193\n",
            "29 0.4854876697063446\n",
            "30 0.47850990295410156\n",
            "31 0.4716328978538513\n",
            "32 0.4648551344871521\n",
            "33 0.458174467086792\n",
            "34 0.4515898823738098\n",
            "35 0.4450993537902832\n",
            "36 0.4387027621269226\n",
            "37 0.43239787220954895\n",
            "38 0.42618393898010254\n",
            "39 0.4200587272644043\n",
            "40 0.41402190923690796\n",
            "41 0.4080716669559479\n",
            "42 0.4022074043750763\n",
            "43 0.39642661809921265\n",
            "44 0.3907294273376465\n",
            "45 0.38511383533477783\n",
            "46 0.3795796036720276\n",
            "47 0.37412407994270325\n",
            "48 0.36874720454216003\n",
            "49 0.3634480834007263\n",
            "50 0.35822468996047974\n",
            "51 0.35307660698890686\n",
            "52 0.34800204634666443\n",
            "53 0.3430008888244629\n",
            "54 0.33807119727134705\n",
            "55 0.33321261405944824\n",
            "56 0.32842394709587097\n",
            "57 0.32370397448539734\n",
            "58 0.31905174255371094\n",
            "59 0.3144666850566864\n",
            "60 0.30994713306427\n",
            "61 0.3054927885532379\n",
            "62 0.3011021018028259\n",
            "63 0.29677513241767883\n",
            "64 0.29250961542129517\n",
            "65 0.28830617666244507\n",
            "66 0.28416261076927185\n",
            "67 0.280078649520874\n",
            "68 0.27605336904525757\n",
            "69 0.27208614349365234\n",
            "70 0.2681759297847748\n",
            "71 0.26432177424430847\n",
            "72 0.2605232000350952\n",
            "73 0.2567790448665619\n",
            "74 0.2530886232852936\n",
            "75 0.2494511902332306\n",
            "76 0.2458663433790207\n",
            "77 0.24233266711235046\n",
            "78 0.23885029554367065\n",
            "79 0.23541760444641113\n",
            "80 0.23203414678573608\n",
            "81 0.22869962453842163\n",
            "82 0.22541262209415436\n",
            "83 0.22217318415641785\n",
            "84 0.21898004412651062\n",
            "85 0.21583296358585358\n",
            "86 0.21273116767406464\n",
            "87 0.20967409014701843\n",
            "88 0.20666062831878662\n",
            "89 0.20369040966033936\n",
            "90 0.20076295733451843\n",
            "91 0.1978779286146164\n",
            "92 0.19503402709960938\n",
            "93 0.19223131239414215\n",
            "94 0.18946856260299683\n",
            "95 0.18674549460411072\n",
            "96 0.18406179547309875\n",
            "97 0.18141630291938782\n",
            "98 0.1788093000650406\n",
            "99 0.17623937129974365\n",
            "100 0.17370672523975372\n",
            "101 0.17121011018753052\n",
            "102 0.16874954104423523\n",
            "103 0.16632431745529175\n",
            "104 0.1639341413974762\n",
            "105 0.161577969789505\n",
            "106 0.15925589203834534\n",
            "107 0.15696710348129272\n",
            "108 0.15471138060092926\n",
            "109 0.15248793363571167\n",
            "110 0.15029644966125488\n",
            "111 0.14813649654388428\n",
            "112 0.1460075080394745\n",
            "113 0.1439090520143509\n",
            "114 0.1418408751487732\n",
            "115 0.13980263471603394\n",
            "116 0.13779345154762268\n",
            "117 0.13581286370754242\n",
            "118 0.13386115431785583\n",
            "119 0.1319374144077301\n",
            "120 0.1300412118434906\n",
            "121 0.1281721442937851\n",
            "122 0.1263302117586136\n",
            "123 0.1245146170258522\n",
            "124 0.12272515892982483\n",
            "125 0.12096142768859863\n",
            "126 0.11922306567430496\n",
            "127 0.11750949919223785\n",
            "128 0.11582063138484955\n",
            "129 0.1141563206911087\n",
            "130 0.11251571774482727\n",
            "131 0.11089867353439331\n",
            "132 0.10930485278367996\n",
            "133 0.1077338233590126\n",
            "134 0.10618560761213303\n",
            "135 0.10465945303440094\n",
            "136 0.10315538942813873\n",
            "137 0.10167300701141357\n",
            "138 0.10021164268255234\n",
            "139 0.09877152740955353\n",
            "140 0.09735213220119476\n",
            "141 0.09595301002264023\n",
            "142 0.09457387030124664\n",
            "143 0.09321475028991699\n",
            "144 0.09187517315149307\n",
            "145 0.09055477380752563\n",
            "146 0.08925336599349976\n",
            "147 0.08797061443328857\n",
            "148 0.08670632541179657\n",
            "149 0.08546019345521927\n",
            "150 0.08423205465078354\n",
            "151 0.08302139490842819\n",
            "152 0.08182825148105621\n",
            "153 0.08065248280763626\n",
            "154 0.07949329912662506\n",
            "155 0.07835068553686142\n",
            "156 0.07722480595111847\n",
            "157 0.0761149600148201\n",
            "158 0.07502096891403198\n",
            "159 0.07394285500049591\n",
            "160 0.07288014143705368\n",
            "161 0.07183285802602768\n",
            "162 0.07080051302909851\n",
            "163 0.06978283077478409\n",
            "164 0.06877996772527695\n",
            "165 0.06779159605503082\n",
            "166 0.06681724637746811\n",
            "167 0.06585696339607239\n",
            "168 0.06491062045097351\n",
            "169 0.06397777795791626\n",
            "170 0.0630582943558693\n",
            "171 0.062152039259672165\n",
            "172 0.06125880032777786\n",
            "173 0.06037841737270355\n",
            "174 0.05951067805290222\n",
            "175 0.058655425906181335\n",
            "176 0.057812318205833435\n",
            "177 0.05698154866695404\n",
            "178 0.05616274103522301\n",
            "179 0.055355511605739594\n",
            "180 0.054559893906116486\n",
            "181 0.053775787353515625\n",
            "182 0.053003035485744476\n",
            "183 0.052241239696741104\n",
            "184 0.051490508019924164\n",
            "185 0.05075046420097351\n",
            "186 0.05002114921808243\n",
            "187 0.04930219426751137\n",
            "188 0.04859374091029167\n",
            "189 0.04789521172642708\n",
            "190 0.04720689356327057\n",
            "191 0.046528518199920654\n",
            "192 0.04585976153612137\n",
            "193 0.04520079120993614\n",
            "194 0.044551149010658264\n",
            "195 0.04391094297170639\n",
            "196 0.04327978938817978\n",
            "197 0.04265771806240082\n",
            "198 0.0420447513461113\n",
            "199 0.041440486907958984\n",
            "200 0.04084489867091179\n",
            "201 0.04025798290967941\n",
            "202 0.03967936336994171\n",
            "203 0.03910915181040764\n",
            "204 0.03854710981249809\n",
            "205 0.03799305856227875\n",
            "206 0.037447087466716766\n",
            "207 0.0369088239967823\n",
            "208 0.0363784059882164\n",
            "209 0.035855621099472046\n",
            "210 0.0353403314948082\n",
            "211 0.03483247756958008\n",
            "212 0.03433184325695038\n",
            "213 0.03383844345808029\n",
            "214 0.033352080732584\n",
            "215 0.03287279233336449\n",
            "216 0.03240039572119713\n",
            "217 0.031934723258018494\n",
            "218 0.03147576376795769\n",
            "219 0.031023383140563965\n",
            "220 0.03057750128209591\n",
            "221 0.030138125643134117\n",
            "222 0.029705066233873367\n",
            "223 0.029278123751282692\n",
            "224 0.028857335448265076\n",
            "225 0.02844257466495037\n",
            "226 0.028033776208758354\n",
            "227 0.027630940079689026\n",
            "228 0.02723386138677597\n",
            "229 0.026842419058084488\n",
            "230 0.02645662985742092\n",
            "231 0.026076363399624825\n",
            "232 0.025701627135276794\n",
            "233 0.025332260876893997\n",
            "234 0.024968229234218597\n",
            "235 0.02460935339331627\n",
            "236 0.02425570972263813\n",
            "237 0.02390713430941105\n",
            "238 0.02356351539492607\n",
            "239 0.023224929347634315\n",
            "240 0.022891202941536903\n",
            "241 0.022562190890312195\n",
            "242 0.022237960249185562\n",
            "243 0.021918322890996933\n",
            "244 0.021603364497423172\n",
            "245 0.02129284292459488\n",
            "246 0.02098681963980198\n",
            "247 0.020685218274593353\n",
            "248 0.020387958735227585\n",
            "249 0.020094959065318108\n",
            "250 0.019806141033768654\n",
            "251 0.01952146366238594\n",
            "252 0.019240982830524445\n",
            "253 0.018964391201734543\n",
            "254 0.01869181916117668\n",
            "255 0.01842321641743183\n",
            "256 0.01815847121179104\n",
            "257 0.01789746806025505\n",
            "258 0.01764027029275894\n",
            "259 0.01738669164478779\n",
            "260 0.017136871814727783\n",
            "261 0.01689056120812893\n",
            "262 0.016647841781377792\n",
            "263 0.016408583149313927\n",
            "264 0.01617274433374405\n",
            "265 0.015940336510539055\n",
            "266 0.01571119949221611\n",
            "267 0.015485407784581184\n",
            "268 0.015262902714312077\n",
            "269 0.015043568797409534\n",
            "270 0.014827393926680088\n",
            "271 0.014614211395382881\n",
            "272 0.014404184184968472\n",
            "273 0.014197228476405144\n",
            "274 0.013993159867823124\n",
            "275 0.01379204448312521\n",
            "276 0.013593869283795357\n",
            "277 0.013398473151028156\n",
            "278 0.013205936178565025\n",
            "279 0.013016123324632645\n",
            "280 0.01282908208668232\n",
            "281 0.012644650414586067\n",
            "282 0.012462976388633251\n",
            "283 0.012283852323889732\n",
            "284 0.012107324786484241\n",
            "285 0.01193330716341734\n",
            "286 0.011761816218495369\n",
            "287 0.011592749506235123\n",
            "288 0.011426156386733055\n",
            "289 0.011261953972280025\n",
            "290 0.011100096628069878\n",
            "291 0.010940544307231903\n",
            "292 0.010783334262669086\n",
            "293 0.010628394782543182\n",
            "294 0.010475625284016132\n",
            "295 0.01032511331140995\n",
            "296 0.010176676325500011\n",
            "297 0.010030407458543777\n",
            "298 0.00988629087805748\n",
            "299 0.009744187816977501\n",
            "300 0.009604137390851974\n",
            "301 0.009466132149100304\n",
            "302 0.009330112487077713\n",
            "303 0.009196009486913681\n",
            "304 0.00906381756067276\n",
            "305 0.00893357302993536\n",
            "306 0.008805165067315102\n",
            "307 0.008678633719682693\n",
            "308 0.008553918451070786\n",
            "309 0.008430995978415012\n",
            "310 0.008309800177812576\n",
            "311 0.008190371096134186\n",
            "312 0.008072668686509132\n",
            "313 0.007956678979098797\n",
            "314 0.0078422911465168\n",
            "315 0.007729621604084969\n",
            "316 0.007618553005158901\n",
            "317 0.007509038783609867\n",
            "318 0.007401148788630962\n",
            "319 0.007294728420674801\n",
            "320 0.007189908064901829\n",
            "321 0.007086569909006357\n",
            "322 0.00698474794626236\n",
            "323 0.006884376518428326\n",
            "324 0.006785407662391663\n",
            "325 0.006687914952635765\n",
            "326 0.006591794081032276\n",
            "327 0.006497090682387352\n",
            "328 0.00640367716550827\n",
            "329 0.0063116238452494144\n",
            "330 0.006220925133675337\n",
            "331 0.006131529342383146\n",
            "332 0.006043430417776108\n",
            "333 0.005956558510661125\n",
            "334 0.005870955530554056\n",
            "335 0.005786565598100424\n",
            "336 0.005703409668058157\n",
            "337 0.005621444433927536\n",
            "338 0.0055406480096280575\n",
            "339 0.0054610250517725945\n",
            "340 0.0053825415670871735\n",
            "341 0.005305193830281496\n",
            "342 0.005228956695646048\n",
            "343 0.005153784528374672\n",
            "344 0.005079720169305801\n",
            "345 0.005006751511245966\n",
            "346 0.004934792406857014\n",
            "347 0.004863869398832321\n",
            "348 0.004793975036591291\n",
            "349 0.004725058097392321\n",
            "350 0.004657152108848095\n",
            "351 0.004590233787894249\n",
            "352 0.004524265415966511\n",
            "353 0.0044592441990971565\n",
            "354 0.004395179450511932\n",
            "355 0.00433197570964694\n",
            "356 0.004269708879292011\n",
            "357 0.004208349157124758\n",
            "358 0.004147890489548445\n",
            "359 0.004088270477950573\n",
            "360 0.004029532428830862\n",
            "361 0.003971600439399481\n",
            "362 0.00391452107578516\n",
            "363 0.0038582663983106613\n",
            "364 0.0038028168492019176\n",
            "365 0.003748163813725114\n",
            "366 0.0036943021696060896\n",
            "367 0.0036412058398127556\n",
            "368 0.0035888804122805595\n",
            "369 0.00353729329071939\n",
            "370 0.003486467758193612\n",
            "371 0.0034363609738647938\n",
            "372 0.003386968746781349\n",
            "373 0.0033382936380803585\n",
            "374 0.0032903174869716167\n",
            "375 0.0032430330757051706\n",
            "376 0.003196409670636058\n",
            "377 0.0031504961661994457\n",
            "378 0.0031052222475409508\n",
            "379 0.003060569055378437\n",
            "380 0.0030165996868163347\n",
            "381 0.00297325337305665\n",
            "382 0.002930525690317154\n",
            "383 0.0028883791528642178\n",
            "384 0.0028468696400523186\n",
            "385 0.0028059659525752068\n",
            "386 0.0027656485326588154\n",
            "387 0.0027258959598839283\n",
            "388 0.0026867056731134653\n",
            "389 0.0026481072418391705\n",
            "390 0.0026100522372871637\n",
            "391 0.0025725441519171\n",
            "392 0.002535562263801694\n",
            "393 0.0024991389364004135\n",
            "394 0.0024632068816572428\n",
            "395 0.002427838509902358\n",
            "396 0.002392922528088093\n",
            "397 0.0023585332091897726\n",
            "398 0.002324636559933424\n",
            "399 0.0022912221029400826\n",
            "400 0.0022582842502743006\n",
            "401 0.0022258455865085125\n",
            "402 0.00219385651871562\n",
            "403 0.0021623235661536455\n",
            "404 0.0021312658209353685\n",
            "405 0.0021006246097385883\n",
            "406 0.002070424845442176\n",
            "407 0.002040680032223463\n",
            "408 0.002011359203606844\n",
            "409 0.0019824523478746414\n",
            "410 0.0019539371132850647\n",
            "411 0.0019258660031482577\n",
            "412 0.001898203045129776\n",
            "413 0.0018709138967096806\n",
            "414 0.0018440193962305784\n",
            "415 0.0018175279255956411\n",
            "416 0.0017913954798132181\n",
            "417 0.0017656453419476748\n",
            "418 0.0017402899684384465\n",
            "419 0.0017152794171124697\n",
            "420 0.0016906221862882376\n",
            "421 0.0016663236310705543\n",
            "422 0.0016423779306933284\n",
            "423 0.0016187832225114107\n",
            "424 0.0015955077251419425\n",
            "425 0.0015725762350484729\n",
            "426 0.0015499708242714405\n",
            "427 0.0015276881167665124\n",
            "428 0.0015057428972795606\n",
            "429 0.0014841086231172085\n",
            "430 0.0014627755153924227\n",
            "431 0.0014417646452784538\n",
            "432 0.0014210306107997894\n",
            "433 0.0014006164856255054\n",
            "434 0.0013804753543809056\n",
            "435 0.0013606431894004345\n",
            "436 0.0013410970568656921\n",
            "437 0.0013218193780630827\n",
            "438 0.0013028195826336741\n",
            "439 0.0012841010466217995\n",
            "440 0.0012656402541324496\n",
            "441 0.0012474484974518418\n",
            "442 0.0012295108754187822\n",
            "443 0.001211856841109693\n",
            "444 0.0011944329598918557\n",
            "445 0.001177274971269071\n",
            "446 0.0011603559833019972\n",
            "447 0.0011436734348535538\n",
            "448 0.0011272503761574626\n",
            "449 0.001111042918637395\n",
            "450 0.0010950644500553608\n",
            "451 0.0010793334804475307\n",
            "452 0.0010638302192091942\n",
            "453 0.0010485406965017319\n",
            "454 0.0010334535036236048\n",
            "455 0.001018603565171361\n",
            "456 0.00100397365167737\n",
            "457 0.0009895554976537824\n",
            "458 0.0009753251797519624\n",
            "459 0.0009613150614313781\n",
            "460 0.0009475084370933473\n",
            "461 0.0009338709060102701\n",
            "462 0.0009204592788591981\n",
            "463 0.0009072321699932218\n",
            "464 0.0008941895212046802\n",
            "465 0.0008813379099592566\n",
            "466 0.0008686616783961654\n",
            "467 0.0008561826543882489\n",
            "468 0.0008438834338448942\n",
            "469 0.0008317604078911245\n",
            "470 0.0008197983843274415\n",
            "471 0.0008080072584562004\n",
            "472 0.0007964016404002905\n",
            "473 0.0007849549874663353\n",
            "474 0.0007736767875030637\n",
            "475 0.0007625509169884026\n",
            "476 0.0007515980978496373\n",
            "477 0.000740797258913517\n",
            "478 0.0007301546866074204\n",
            "479 0.0007196530932560563\n",
            "480 0.0007093142485246062\n",
            "481 0.0006991241243667901\n",
            "482 0.0006890824297443032\n",
            "483 0.0006791754858568311\n",
            "484 0.0006694126059301198\n",
            "485 0.0006597815081477165\n",
            "486 0.0006503078038804233\n",
            "487 0.0006409672787413001\n",
            "488 0.0006317498628050089\n",
            "489 0.0006226616096682847\n",
            "490 0.0006137184100225568\n",
            "491 0.0006049114745110273\n",
            "492 0.000596212106756866\n",
            "493 0.0005876420764252543\n",
            "494 0.0005791899748146534\n",
            "495 0.0005708575481548905\n",
            "496 0.0005626626079902053\n",
            "497 0.0005545694148167968\n",
            "498 0.00054660823661834\n",
            "499 0.0005387560231611133\n",
            "predict (after training) 4 tensor(7.9733)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8eSbBFXCnH7",
        "outputId": "1996c0f2-b956-4607-9e1e-a24f34c4a198"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "x_data = Variable(torch.Tensor([[1.0], [2.0], [3.0], [4.0]]))\n",
        "y_data = Variable(torch.Tensor([[0.], [0.], [1.], [1.]]))\n",
        "\n",
        "class Model(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Model, self).__init__()\n",
        "    self.linear=torch.nn.Linear(1,1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    y_pred = F.sigmoid(self.linear(x))\n",
        "    return y_pred\n",
        "\n",
        "model = Model()\n",
        "\n",
        "criterion = torch.nn.BCELoss(size_average=True)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "for epoch in range(1000):\n",
        "  y_pred = model(x_data)\n",
        "\n",
        "  loss = criterion(y_pred, y_data)\n",
        "  print(epoch, loss.item())\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "hour_var = Variable(torch.Tensor([[1.0]]))\n",
        "print(\"predict i hour\", 1.0, model(hour_var).data[0][0] > 0.5)\n",
        "hour_var = Variable(torch.Tensor([[7.0]]))\n",
        "print(\"predict 7 hour\", 7.0, model(hour_var).data[0][0] > 0.5)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0 0.7475818991661072\n",
            "1 0.7460820078849792\n",
            "2 0.7446115612983704\n",
            "3 0.7431700229644775\n",
            "4 0.7417568564414978\n",
            "5 0.7403714656829834\n",
            "6 0.7390135526657104\n",
            "7 0.7376824021339417\n",
            "8 0.736377477645874\n",
            "9 0.7350984215736389\n",
            "10 0.7338448166847229\n",
            "11 0.7326157689094543\n",
            "12 0.7314112186431885\n",
            "13 0.7302303910255432\n",
            "14 0.7290728688240051\n",
            "15 0.7279382348060608\n",
            "16 0.7268259525299072\n",
            "17 0.7257355451583862\n",
            "18 0.7246665954589844\n",
            "19 0.7236184477806091\n",
            "20 0.7225909233093262\n",
            "21 0.7215834259986877\n",
            "22 0.7205954790115356\n",
            "23 0.7196266651153564\n",
            "24 0.7186764478683472\n",
            "25 0.7177445888519287\n",
            "26 0.7168306708335876\n",
            "27 0.715934157371521\n",
            "28 0.7150545120239258\n",
            "29 0.7141916751861572\n",
            "30 0.7133448719978333\n",
            "31 0.7125139832496643\n",
            "32 0.7116984128952026\n",
            "33 0.7108978629112244\n",
            "34 0.7101122140884399\n",
            "35 0.7093406915664673\n",
            "36 0.7085831761360168\n",
            "37 0.70783931016922\n",
            "38 0.7071085572242737\n",
            "39 0.7063906788825989\n",
            "40 0.7056854963302612\n",
            "41 0.7049925923347473\n",
            "42 0.7043114900588989\n",
            "43 0.7036421895027161\n",
            "44 0.7029839158058167\n",
            "45 0.7023367881774902\n",
            "46 0.7017003893852234\n",
            "47 0.7010744214057922\n",
            "48 0.7004585266113281\n",
            "49 0.6998526453971863\n",
            "50 0.6992563009262085\n",
            "51 0.6986691355705261\n",
            "52 0.6980911493301392\n",
            "53 0.6975221037864685\n",
            "54 0.696961522102356\n",
            "55 0.696409285068512\n",
            "56 0.6958652138710022\n",
            "57 0.695328950881958\n",
            "58 0.6948004364967346\n",
            "59 0.6942793130874634\n",
            "60 0.69376540184021\n",
            "61 0.69325852394104\n",
            "62 0.6927585005760193\n",
            "63 0.6922651529312134\n",
            "64 0.6917781233787537\n",
            "65 0.6912974119186401\n",
            "66 0.6908227205276489\n",
            "67 0.6903541684150696\n",
            "68 0.6898910403251648\n",
            "69 0.6894335746765137\n",
            "70 0.6889815330505371\n",
            "71 0.6885347366333008\n",
            "72 0.6880929470062256\n",
            "73 0.6876562237739563\n",
            "74 0.6872242093086243\n",
            "75 0.6867968440055847\n",
            "76 0.6863740086555481\n",
            "77 0.6859555244445801\n",
            "78 0.6855414509773254\n",
            "79 0.6851314306259155\n",
            "80 0.6847253441810608\n",
            "81 0.6843230724334717\n",
            "82 0.6839246153831482\n",
            "83 0.6835299730300903\n",
            "84 0.6831386685371399\n",
            "85 0.682750940322876\n",
            "86 0.6823664903640747\n",
            "87 0.6819853186607361\n",
            "88 0.6816073060035706\n",
            "89 0.6812322735786438\n",
            "90 0.6808602213859558\n",
            "91 0.680491030216217\n",
            "92 0.6801246404647827\n",
            "93 0.6797609329223633\n",
            "94 0.6793999075889587\n",
            "95 0.6790413856506348\n",
            "96 0.6786853075027466\n",
            "97 0.6783316731452942\n",
            "98 0.677980363368988\n",
            "99 0.6776312589645386\n",
            "100 0.6772844195365906\n",
            "101 0.6769396066665649\n",
            "102 0.6765968799591064\n",
            "103 0.6762561202049255\n",
            "104 0.6759173274040222\n",
            "105 0.6755805015563965\n",
            "106 0.6752453446388245\n",
            "107 0.6749120950698853\n",
            "108 0.674580454826355\n",
            "109 0.674250602722168\n",
            "110 0.6739223003387451\n",
            "111 0.6735956072807312\n",
            "112 0.6732704639434814\n",
            "113 0.6729467511177063\n",
            "114 0.6726245880126953\n",
            "115 0.6723036170005798\n",
            "116 0.6719841361045837\n",
            "117 0.6716659069061279\n",
            "118 0.671349048614502\n",
            "119 0.6710334420204163\n",
            "120 0.6707189679145813\n",
            "121 0.6704057455062866\n",
            "122 0.6700936555862427\n",
            "123 0.6697826385498047\n",
            "124 0.6694728136062622\n",
            "125 0.6691638827323914\n",
            "126 0.6688560247421265\n",
            "127 0.6685492396354675\n",
            "128 0.668243408203125\n",
            "129 0.6679384112358093\n",
            "130 0.6676344275474548\n",
            "131 0.6673312187194824\n",
            "132 0.667029082775116\n",
            "133 0.666727602481842\n",
            "134 0.6664271354675293\n",
            "135 0.6661272644996643\n",
            "136 0.6658282279968262\n",
            "137 0.6655300855636597\n",
            "138 0.6652325391769409\n",
            "139 0.664935827255249\n",
            "140 0.6646398305892944\n",
            "141 0.6643444299697876\n",
            "142 0.6640496850013733\n",
            "143 0.6637556552886963\n",
            "144 0.663462221622467\n",
            "145 0.6631695032119751\n",
            "146 0.6628773212432861\n",
            "147 0.6625856161117554\n",
            "148 0.6622946858406067\n",
            "149 0.6620042324066162\n",
            "150 0.6617144346237183\n",
            "151 0.6614251136779785\n",
            "152 0.6611363291740417\n",
            "153 0.660848081111908\n",
            "154 0.6605602502822876\n",
            "155 0.660273015499115\n",
            "156 0.6599861979484558\n",
            "157 0.6596997976303101\n",
            "158 0.6594139933586121\n",
            "159 0.659128725528717\n",
            "160 0.6588436365127563\n",
            "161 0.6585592031478882\n",
            "162 0.6582751274108887\n",
            "163 0.6579914093017578\n",
            "164 0.6577082276344299\n",
            "165 0.6574253439903259\n",
            "166 0.6571429371833801\n",
            "167 0.656860888004303\n",
            "168 0.6565791368484497\n",
            "169 0.6562979817390442\n",
            "170 0.656017005443573\n",
            "171 0.6557364463806152\n",
            "172 0.6554562449455261\n",
            "173 0.6551764011383057\n",
            "174 0.6548969745635986\n",
            "175 0.6546177864074707\n",
            "176 0.6543389558792114\n",
            "177 0.654060423374176\n",
            "178 0.6537822484970093\n",
            "179 0.6535044312477112\n",
            "180 0.653226912021637\n",
            "181 0.6529497504234314\n",
            "182 0.6526728868484497\n",
            "183 0.6523962616920471\n",
            "184 0.6521199345588684\n",
            "185 0.6518439054489136\n",
            "186 0.6515682935714722\n",
            "187 0.6512928009033203\n",
            "188 0.6510176658630371\n",
            "189 0.6507428288459778\n",
            "190 0.6504682302474976\n",
            "191 0.6501939296722412\n",
            "192 0.649919867515564\n",
            "193 0.6496461629867554\n",
            "194 0.6493727564811707\n",
            "195 0.6490994095802307\n",
            "196 0.6488263607025146\n",
            "197 0.648553729057312\n",
            "198 0.6482812166213989\n",
            "199 0.6480090022087097\n",
            "200 0.6477370262145996\n",
            "201 0.6474653482437134\n",
            "202 0.6471938490867615\n",
            "203 0.6469227075576782\n",
            "204 0.6466516256332397\n",
            "205 0.6463809609413147\n",
            "206 0.6461104154586792\n",
            "207 0.6458401083946228\n",
            "208 0.6455700397491455\n",
            "209 0.6453003287315369\n",
            "210 0.6450307965278625\n",
            "211 0.6447613835334778\n",
            "212 0.6444923281669617\n",
            "213 0.6442234516143799\n",
            "214 0.6439547538757324\n",
            "215 0.6436863541603088\n",
            "216 0.6434181332588196\n",
            "217 0.6431500911712646\n",
            "218 0.6428822875022888\n",
            "219 0.6426147818565369\n",
            "220 0.6423473954200745\n",
            "221 0.6420803070068359\n",
            "222 0.6418133974075317\n",
            "223 0.6415466666221619\n",
            "224 0.6412801742553711\n",
            "225 0.6410138607025146\n",
            "226 0.6407478451728821\n",
            "227 0.6404819488525391\n",
            "228 0.6402163505554199\n",
            "229 0.6399508118629456\n",
            "230 0.6396855711936951\n",
            "231 0.6394205689430237\n",
            "232 0.6391557455062866\n",
            "233 0.6388911604881287\n",
            "234 0.6386266946792603\n",
            "235 0.638362467288971\n",
            "236 0.638098418712616\n",
            "237 0.6378345489501953\n",
            "238 0.6375710368156433\n",
            "239 0.6373076438903809\n",
            "240 0.6370443105697632\n",
            "241 0.6367812752723694\n",
            "242 0.6365185379981995\n",
            "243 0.6362559199333191\n",
            "244 0.635993480682373\n",
            "245 0.6357311606407166\n",
            "246 0.6354691982269287\n",
            "247 0.6352072954177856\n",
            "248 0.6349456310272217\n",
            "249 0.634684145450592\n",
            "250 0.6344228982925415\n",
            "251 0.6341618299484253\n",
            "252 0.6339009404182434\n",
            "253 0.6336402297019958\n",
            "254 0.6333798170089722\n",
            "255 0.6331194043159485\n",
            "256 0.6328593492507935\n",
            "257 0.6325993537902832\n",
            "258 0.632339596748352\n",
            "259 0.632080078125\n",
            "260 0.6318207383155823\n",
            "261 0.6315615773200989\n",
            "262 0.631302535533905\n",
            "263 0.6310437917709351\n",
            "264 0.6307850480079651\n",
            "265 0.6305267214775085\n",
            "266 0.630268394947052\n",
            "267 0.6300103664398193\n",
            "268 0.629752516746521\n",
            "269 0.629494845867157\n",
            "270 0.6292373538017273\n",
            "271 0.6289799809455872\n",
            "272 0.6287228465080261\n",
            "273 0.6284659504890442\n",
            "274 0.6282091736793518\n",
            "275 0.6279526352882385\n",
            "276 0.62769615650177\n",
            "277 0.6274399757385254\n",
            "278 0.6271839737892151\n",
            "279 0.6269280910491943\n",
            "280 0.6266724467277527\n",
            "281 0.6264169812202454\n",
            "282 0.6261616945266724\n",
            "283 0.6259065270423889\n",
            "284 0.6256515979766846\n",
            "285 0.6253967881202698\n",
            "286 0.6251422762870789\n",
            "287 0.6248878240585327\n",
            "288 0.6246336102485657\n",
            "289 0.6243796348571777\n",
            "290 0.6241257786750793\n",
            "291 0.6238721013069153\n",
            "292 0.6236186623573303\n",
            "293 0.6233652830123901\n",
            "294 0.6231120824813843\n",
            "295 0.6228591799736023\n",
            "296 0.6226063966751099\n",
            "297 0.6223538517951965\n",
            "298 0.622101366519928\n",
            "299 0.6218491196632385\n",
            "300 0.6215970516204834\n",
            "301 0.6213452219963074\n",
            "302 0.6210934519767761\n",
            "303 0.620841920375824\n",
            "304 0.6205905675888062\n",
            "305 0.6203393936157227\n",
            "306 0.6200883984565735\n",
            "307 0.6198376417160034\n",
            "308 0.6195869445800781\n",
            "309 0.6193364858627319\n",
            "310 0.6190862059593201\n",
            "311 0.6188360452651978\n",
            "312 0.6185860633850098\n",
            "313 0.6183363199234009\n",
            "314 0.6180867552757263\n",
            "315 0.6178373694419861\n",
            "316 0.6175881028175354\n",
            "317 0.6173390746116638\n",
            "318 0.6170901656150818\n",
            "319 0.6168414950370789\n",
            "320 0.6165928840637207\n",
            "321 0.6163445711135864\n",
            "322 0.6160964369773865\n",
            "323 0.6158483624458313\n",
            "324 0.6156005859375\n",
            "325 0.6153528690338135\n",
            "326 0.615105390548706\n",
            "327 0.614858090877533\n",
            "328 0.6146109700202942\n",
            "329 0.6143639087677002\n",
            "330 0.6141171455383301\n",
            "331 0.6138705015182495\n",
            "332 0.6136240363121033\n",
            "333 0.6133778095245361\n",
            "334 0.6131317019462585\n",
            "335 0.6128857135772705\n",
            "336 0.6126399636268616\n",
            "337 0.6123944520950317\n",
            "338 0.6121490001678467\n",
            "339 0.611903727054596\n",
            "340 0.6116586923599243\n",
            "341 0.6114137172698975\n",
            "342 0.6111690998077393\n",
            "343 0.610924482345581\n",
            "344 0.6106801629066467\n",
            "345 0.6104359030723572\n",
            "346 0.6101918816566467\n",
            "347 0.6099480390548706\n",
            "348 0.6097042560577393\n",
            "349 0.6094607710838318\n",
            "350 0.6092174649238586\n",
            "351 0.6089742183685303\n",
            "352 0.608731210231781\n",
            "353 0.6084884405136108\n",
            "354 0.6082456707954407\n",
            "355 0.6080031991004944\n",
            "356 0.6077609062194824\n",
            "357 0.60751873254776\n",
            "358 0.6072766780853271\n",
            "359 0.6070348620414734\n",
            "360 0.606793224811554\n",
            "361 0.6065517067909241\n",
            "362 0.6063104867935181\n",
            "363 0.6060692667961121\n",
            "364 0.6058282852172852\n",
            "365 0.6055875420570374\n",
            "366 0.6053467988967896\n",
            "367 0.6051063537597656\n",
            "368 0.6048660278320312\n",
            "369 0.604625940322876\n",
            "370 0.6043859124183655\n",
            "371 0.6041461825370789\n",
            "372 0.6039066314697266\n",
            "373 0.6036670804023743\n",
            "374 0.6034278273582458\n",
            "375 0.6031886339187622\n",
            "376 0.6029497385025024\n",
            "377 0.6027109026908875\n",
            "378 0.6024723052978516\n",
            "379 0.6022338271141052\n",
            "380 0.6019955277442932\n",
            "381 0.6017574071884155\n",
            "382 0.6015194654464722\n",
            "383 0.6012816429138184\n",
            "384 0.6010441184043884\n",
            "385 0.6008065938949585\n",
            "386 0.6005693078041077\n",
            "387 0.6003321409225464\n",
            "388 0.6000952124595642\n",
            "389 0.5998584032058716\n",
            "390 0.5996218323707581\n",
            "391 0.5993853807449341\n",
            "392 0.5991491079330444\n",
            "393 0.5989129543304443\n",
            "394 0.5986769795417786\n",
            "395 0.5984411835670471\n",
            "396 0.5982055068016052\n",
            "397 0.5979700684547424\n",
            "398 0.5977347493171692\n",
            "399 0.597499668598175\n",
            "400 0.5972646474838257\n",
            "401 0.5970298647880554\n",
            "402 0.5967952013015747\n",
            "403 0.5965606570243835\n",
            "404 0.5963263511657715\n",
            "405 0.5960922241210938\n",
            "406 0.5958582758903503\n",
            "407 0.5956244468688965\n",
            "408 0.5953907370567322\n",
            "409 0.5951573252677917\n",
            "410 0.5949239134788513\n",
            "411 0.5946907997131348\n",
            "412 0.594457745552063\n",
            "413 0.5942249894142151\n",
            "414 0.593992292881012\n",
            "415 0.5937597155570984\n",
            "416 0.5935274362564087\n",
            "417 0.5932950973510742\n",
            "418 0.5930631756782532\n",
            "419 0.5928313136100769\n",
            "420 0.5925995707511902\n",
            "421 0.5923680663108826\n",
            "422 0.5921366810798645\n",
            "423 0.5919054746627808\n",
            "424 0.5916743278503418\n",
            "425 0.5914434790611267\n",
            "426 0.591212809085846\n",
            "427 0.59098219871521\n",
            "428 0.5907517671585083\n",
            "429 0.590521514415741\n",
            "430 0.5902913808822632\n",
            "431 0.5900614857673645\n",
            "432 0.5898318290710449\n",
            "433 0.5896021723747253\n",
            "434 0.5893726944923401\n",
            "435 0.5891434550285339\n",
            "436 0.5889142751693726\n",
            "437 0.5886853337287903\n",
            "438 0.5884565114974976\n",
            "439 0.5882278680801392\n",
            "440 0.5879994034767151\n",
            "441 0.5877710580825806\n",
            "442 0.5875429511070251\n",
            "443 0.5873149633407593\n",
            "444 0.5870870351791382\n",
            "445 0.586859405040741\n",
            "446 0.5866318941116333\n",
            "447 0.5864045023918152\n",
            "448 0.5861773490905762\n",
            "449 0.5859502553939819\n",
            "450 0.5857232809066772\n",
            "451 0.5854966640472412\n",
            "452 0.5852700471878052\n",
            "453 0.5850436687469482\n",
            "454 0.5848174095153809\n",
            "455 0.5845913290977478\n",
            "456 0.5843653678894043\n",
            "457 0.5841395854949951\n",
            "458 0.583914041519165\n",
            "459 0.583688497543335\n",
            "460 0.5834632515907288\n",
            "461 0.5832380652427673\n",
            "462 0.583013117313385\n",
            "463 0.5827882289886475\n",
            "464 0.582563579082489\n",
            "465 0.5823390483856201\n",
            "466 0.5821146965026855\n",
            "467 0.5818905234336853\n",
            "468 0.5816664695739746\n",
            "469 0.581442654132843\n",
            "470 0.5812187790870667\n",
            "471 0.5809952616691589\n",
            "472 0.580771803855896\n",
            "473 0.5805485248565674\n",
            "474 0.5803254842758179\n",
            "475 0.5801025032997131\n",
            "476 0.579879641532898\n",
            "477 0.5796570777893066\n",
            "478 0.5794345736503601\n",
            "479 0.5792121887207031\n",
            "480 0.5789900422096252\n",
            "481 0.5787680149078369\n",
            "482 0.5785462260246277\n",
            "483 0.5783244967460632\n",
            "484 0.5781029462814331\n",
            "485 0.5778815746307373\n",
            "486 0.5776602625846863\n",
            "487 0.5774391889572144\n",
            "488 0.577218234539032\n",
            "489 0.5769974589347839\n",
            "490 0.5767768621444702\n",
            "491 0.576556384563446\n",
            "492 0.5763360261917114\n",
            "493 0.5761159062385559\n",
            "494 0.5758959054946899\n",
            "495 0.5756760239601135\n",
            "496 0.5754563212394714\n",
            "497 0.5752367973327637\n",
            "498 0.5750173926353455\n",
            "499 0.5747981667518616\n",
            "500 0.5745790600776672\n",
            "501 0.5743600726127625\n",
            "502 0.5741413831710815\n",
            "503 0.5739227533340454\n",
            "504 0.5737042427062988\n",
            "505 0.5734859704971313\n",
            "506 0.5732676982879639\n",
            "507 0.573049783706665\n",
            "508 0.5728318691253662\n",
            "509 0.5726140737533569\n",
            "510 0.5723965764045715\n",
            "511 0.5721792578697205\n",
            "512 0.5719619393348694\n",
            "513 0.5717448592185974\n",
            "514 0.5715278387069702\n",
            "515 0.5713110566139221\n",
            "516 0.5710944533348083\n",
            "517 0.5708779692649841\n",
            "518 0.5706615447998047\n",
            "519 0.5704454183578491\n",
            "520 0.5702293515205383\n",
            "521 0.5700134634971619\n",
            "522 0.5697977542877197\n",
            "523 0.5695821046829224\n",
            "524 0.5693666934967041\n",
            "525 0.5691514015197754\n",
            "526 0.5689362287521362\n",
            "527 0.5687212347984314\n",
            "528 0.5685063600540161\n",
            "529 0.5682916641235352\n",
            "530 0.5680772066116333\n",
            "531 0.5678627490997314\n",
            "532 0.5676485300064087\n",
            "533 0.5674344301223755\n",
            "534 0.5672204494476318\n",
            "535 0.5670067071914673\n",
            "536 0.5667930841445923\n",
            "537 0.5665795207023621\n",
            "538 0.5663661956787109\n",
            "539 0.5661530494689941\n",
            "540 0.5659399628639221\n",
            "541 0.5657271146774292\n",
            "542 0.5655142664909363\n",
            "543 0.5653017163276672\n",
            "544 0.565089225769043\n",
            "545 0.564876914024353\n",
            "546 0.5646647810935974\n",
            "547 0.5644527673721313\n",
            "548 0.5642409324645996\n",
            "549 0.5640292167663574\n",
            "550 0.5638176202774048\n",
            "551 0.5636061429977417\n",
            "552 0.5633949041366577\n",
            "553 0.5631837844848633\n",
            "554 0.5629727840423584\n",
            "555 0.5627619028091431\n",
            "556 0.5625513195991516\n",
            "557 0.5623407363891602\n",
            "558 0.562130331993103\n",
            "559 0.5619201064109802\n",
            "560 0.5617100596427917\n",
            "561 0.5615000128746033\n",
            "562 0.5612902641296387\n",
            "563 0.5610805749893188\n",
            "564 0.5608710646629333\n",
            "565 0.5606616735458374\n",
            "566 0.5604524612426758\n",
            "567 0.5602434873580933\n",
            "568 0.560034453868866\n",
            "569 0.5598257184028625\n",
            "570 0.5596171021461487\n",
            "571 0.5594086050987244\n",
            "572 0.5592002272605896\n",
            "573 0.5589920282363892\n",
            "574 0.5587839484214783\n",
            "575 0.5585760474205017\n",
            "576 0.5583683252334595\n",
            "577 0.558160662651062\n",
            "578 0.5579531788825989\n",
            "579 0.5577458143234253\n",
            "580 0.557538628578186\n",
            "581 0.5573315620422363\n",
            "582 0.5571247339248657\n",
            "583 0.5569179058074951\n",
            "584 0.5567113161087036\n",
            "585 0.5565049052238464\n",
            "586 0.5562984943389893\n",
            "587 0.5560923218727112\n",
            "588 0.5558862686157227\n",
            "589 0.5556803941726685\n",
            "590 0.5554746389389038\n",
            "591 0.5552690029144287\n",
            "592 0.5550635457038879\n",
            "593 0.5548582673072815\n",
            "594 0.5546530485153198\n",
            "595 0.5544480681419373\n",
            "596 0.5542431473731995\n",
            "597 0.5540383458137512\n",
            "598 0.5538337826728821\n",
            "599 0.5536293983459473\n",
            "600 0.5534249544143677\n",
            "601 0.5532208681106567\n",
            "602 0.5530167818069458\n",
            "603 0.5528128147125244\n",
            "604 0.5526090860366821\n",
            "605 0.5524054765701294\n",
            "606 0.5522019863128662\n",
            "607 0.5519987344741821\n",
            "608 0.5517954230308533\n",
            "609 0.5515925288200378\n",
            "610 0.5513895153999329\n",
            "611 0.5511868000030518\n",
            "612 0.5509841442108154\n",
            "613 0.5507816076278687\n",
            "614 0.550579309463501\n",
            "615 0.5503771305084229\n",
            "616 0.5501750111579895\n",
            "617 0.5499731302261353\n",
            "618 0.5497713088989258\n",
            "619 0.5495696067810059\n",
            "620 0.549368143081665\n",
            "621 0.5491667985916138\n",
            "622 0.548965573310852\n",
            "623 0.5487644672393799\n",
            "624 0.548563539981842\n",
            "625 0.5483627319335938\n",
            "626 0.5481619834899902\n",
            "627 0.5479615330696106\n",
            "628 0.547761082649231\n",
            "629 0.5475609302520752\n",
            "630 0.5473607778549194\n",
            "631 0.547160804271698\n",
            "632 0.5469609498977661\n",
            "633 0.5467612743377686\n",
            "634 0.5465617179870605\n",
            "635 0.5463622808456421\n",
            "636 0.5461630821228027\n",
            "637 0.5459638833999634\n",
            "638 0.5457649230957031\n",
            "639 0.5455660223960876\n",
            "640 0.5453672409057617\n",
            "641 0.5451686978340149\n",
            "642 0.5449702739715576\n",
            "643 0.5447719693183899\n",
            "644 0.5445737242698669\n",
            "645 0.5443757176399231\n",
            "646 0.5441778302192688\n",
            "647 0.543980062007904\n",
            "648 0.5437824726104736\n",
            "649 0.543584942817688\n",
            "650 0.5433875918388367\n",
            "651 0.5431903600692749\n",
            "652 0.5429932475090027\n",
            "653 0.5427963733673096\n",
            "654 0.5425994992256165\n",
            "655 0.5424028635025024\n",
            "656 0.542206346988678\n",
            "657 0.5420099496841431\n",
            "658 0.5418137311935425\n",
            "659 0.5416175723075867\n",
            "660 0.5414215922355652\n",
            "661 0.5412257313728333\n",
            "662 0.5410299897193909\n",
            "663 0.540834367275238\n",
            "664 0.5406389236450195\n",
            "665 0.5404437184333801\n",
            "666 0.540248453617096\n",
            "667 0.5400534272193909\n",
            "668 0.5398585200309753\n",
            "669 0.5396637916564941\n",
            "670 0.5394691228866577\n",
            "671 0.5392745733261108\n",
            "672 0.5390802621841431\n",
            "673 0.5388859510421753\n",
            "674 0.5386918187141418\n",
            "675 0.5384979248046875\n",
            "676 0.5383040308952332\n",
            "677 0.5381104350090027\n",
            "678 0.5379168391227722\n",
            "679 0.5377234220504761\n",
            "680 0.5375300645828247\n",
            "681 0.5373368859291077\n",
            "682 0.5371439456939697\n",
            "683 0.5369510054588318\n",
            "684 0.5367581844329834\n",
            "685 0.5365656018257141\n",
            "686 0.5363731384277344\n",
            "687 0.5361807942390442\n",
            "688 0.535988450050354\n",
            "689 0.5357964038848877\n",
            "690 0.5356044769287109\n",
            "691 0.535412609577179\n",
            "692 0.5352209806442261\n",
            "693 0.5350293517112732\n",
            "694 0.5348378419876099\n",
            "695 0.5346466302871704\n",
            "696 0.534455418586731\n",
            "697 0.5342644453048706\n",
            "698 0.534073531627655\n",
            "699 0.533882737159729\n",
            "700 0.5336920619010925\n",
            "701 0.5335015654563904\n",
            "702 0.533311128616333\n",
            "703 0.5331209301948547\n",
            "704 0.5329309105873108\n",
            "705 0.5327408313751221\n",
            "706 0.5325510501861572\n",
            "707 0.5323613286018372\n",
            "708 0.5321716666221619\n",
            "709 0.5319822430610657\n",
            "710 0.5317928791046143\n",
            "711 0.5316036939620972\n",
            "712 0.5314146280288696\n",
            "713 0.5312256217002869\n",
            "714 0.5310368537902832\n",
            "715 0.5308481454849243\n",
            "716 0.5306596159934998\n",
            "717 0.5304712057113647\n",
            "718 0.5302828550338745\n",
            "719 0.5300947427749634\n",
            "720 0.5299067497253418\n",
            "721 0.5297187566757202\n",
            "722 0.5295310616493225\n",
            "723 0.5293433666229248\n",
            "724 0.5291558504104614\n",
            "725 0.5289685130119324\n",
            "726 0.5287811756134033\n",
            "727 0.5285940766334534\n",
            "728 0.528407096862793\n",
            "729 0.5282201766967773\n",
            "730 0.5280334949493408\n",
            "731 0.5278468728065491\n",
            "732 0.5276604294776917\n",
            "733 0.5274741053581238\n",
            "734 0.5272878408432007\n",
            "735 0.5271016955375671\n",
            "736 0.5269157290458679\n",
            "737 0.526729941368103\n",
            "738 0.5265441536903381\n",
            "739 0.5263586044311523\n",
            "740 0.5261731147766113\n",
            "741 0.5259877443313599\n",
            "742 0.5258026123046875\n",
            "743 0.5256174206733704\n",
            "744 0.5254325270652771\n",
            "745 0.5252477526664734\n",
            "746 0.5250630378723145\n",
            "747 0.5248784422874451\n",
            "748 0.52469402551651\n",
            "749 0.5245096683502197\n",
            "750 0.5243254899978638\n",
            "751 0.5241413712501526\n",
            "752 0.5239574909210205\n",
            "753 0.5237736701965332\n",
            "754 0.5235900282859802\n",
            "755 0.523406445980072\n",
            "756 0.5232230424880981\n",
            "757 0.523039698600769\n",
            "758 0.5228565335273743\n",
            "759 0.5226734280586243\n",
            "760 0.5224905610084534\n",
            "761 0.5223076939582825\n",
            "762 0.5221250653266907\n",
            "763 0.5219424962997437\n",
            "764 0.5217600464820862\n",
            "765 0.5215777158737183\n",
            "766 0.5213955640792847\n",
            "767 0.5212135314941406\n",
            "768 0.5210315585136414\n",
            "769 0.5208497643470764\n",
            "770 0.520668089389801\n",
            "771 0.5204865336418152\n",
            "772 0.5203051567077637\n",
            "773 0.5201237797737122\n",
            "774 0.5199426412582397\n",
            "775 0.5197615623474121\n",
            "776 0.519580602645874\n",
            "777 0.5193998217582703\n",
            "778 0.5192191004753113\n",
            "779 0.5190385580062866\n",
            "780 0.5188581943511963\n",
            "781 0.518677830696106\n",
            "782 0.5184975862503052\n",
            "783 0.5183175802230835\n",
            "784 0.5181376338005066\n",
            "785 0.5179578065872192\n",
            "786 0.5177780985832214\n",
            "787 0.5175985097885132\n",
            "788 0.5174190402030945\n",
            "789 0.5172396898269653\n",
            "790 0.5170605182647705\n",
            "791 0.5168814063072205\n",
            "792 0.5167024731636047\n",
            "793 0.5165236592292786\n",
            "794 0.5163449645042419\n",
            "795 0.5161662697792053\n",
            "796 0.5159878730773926\n",
            "797 0.5158094763755798\n",
            "798 0.5156312584877014\n",
            "799 0.5154531598091125\n",
            "800 0.5152751803398132\n",
            "801 0.5150972604751587\n",
            "802 0.5149195790290833\n",
            "803 0.5147418975830078\n",
            "804 0.5145644545555115\n",
            "805 0.5143870711326599\n",
            "806 0.5142096877098083\n",
            "807 0.5140326619148254\n",
            "808 0.5138556957244873\n",
            "809 0.513678789138794\n",
            "810 0.5135019421577454\n",
            "811 0.5133252739906311\n",
            "812 0.5131487846374512\n",
            "813 0.512972354888916\n",
            "814 0.5127959847450256\n",
            "815 0.5126198530197144\n",
            "816 0.5124438405036926\n",
            "817 0.5122678875923157\n",
            "818 0.5120920538902283\n",
            "819 0.5119163393974304\n",
            "820 0.5117407441139221\n",
            "821 0.5115653872489929\n",
            "822 0.5113900303840637\n",
            "823 0.5112147927284241\n",
            "824 0.5110396146774292\n",
            "825 0.5108647346496582\n",
            "826 0.5106898546218872\n",
            "827 0.5105150938034058\n",
            "828 0.5103405117988586\n",
            "829 0.5101659893989563\n",
            "830 0.5099916458129883\n",
            "831 0.509817361831665\n",
            "832 0.5096431970596313\n",
            "833 0.509469211101532\n",
            "834 0.5092952251434326\n",
            "835 0.5091215372085571\n",
            "836 0.5089477896690369\n",
            "837 0.5087742805480957\n",
            "838 0.5086008310317993\n",
            "839 0.5084275007247925\n",
            "840 0.5082542896270752\n",
            "841 0.5080811977386475\n",
            "842 0.5079082250595093\n",
            "843 0.5077354311943054\n",
            "844 0.5075626373291016\n",
            "845 0.507390022277832\n",
            "846 0.507217526435852\n",
            "847 0.5070452094078064\n",
            "848 0.5068729519844055\n",
            "849 0.5067007541656494\n",
            "850 0.5065287351608276\n",
            "851 0.5063568949699402\n",
            "852 0.5061850547790527\n",
            "853 0.5060133934020996\n",
            "854 0.505841851234436\n",
            "855 0.5056703686714172\n",
            "856 0.5054990649223328\n",
            "857 0.5053278207778931\n",
            "858 0.5051567554473877\n",
            "859 0.5049858093261719\n",
            "860 0.504814863204956\n",
            "861 0.5046442151069641\n",
            "862 0.5044735670089722\n",
            "863 0.5043030381202698\n",
            "864 0.5041326284408569\n",
            "865 0.5039622783660889\n",
            "866 0.5037921667098999\n",
            "867 0.5036221146583557\n",
            "868 0.5034521818161011\n",
            "869 0.503282368183136\n",
            "870 0.5031126737594604\n",
            "871 0.5029430389404297\n",
            "872 0.5027735233306885\n",
            "873 0.5026042461395264\n",
            "874 0.5024349689483643\n",
            "875 0.5022658705711365\n",
            "876 0.5020967721939087\n",
            "877 0.50192791223526\n",
            "878 0.5017591118812561\n",
            "879 0.5015904307365417\n",
            "880 0.5014218688011169\n",
            "881 0.5012534260749817\n",
            "882 0.501085102558136\n",
            "883 0.5009168386459351\n",
            "884 0.5007487535476685\n",
            "885 0.5005807280540466\n",
            "886 0.5004128813743591\n",
            "887 0.5002450942993164\n",
            "888 0.5000774264335632\n",
            "889 0.4999099373817444\n",
            "890 0.4997425079345703\n",
            "891 0.499575138092041\n",
            "892 0.49940794706344604\n",
            "893 0.4992408752441406\n",
            "894 0.49907392263412476\n",
            "895 0.49890702962875366\n",
            "896 0.4987402558326721\n",
            "897 0.4985736608505249\n",
            "898 0.49840718507766724\n",
            "899 0.4982406795024872\n",
            "900 0.4980744421482086\n",
            "901 0.49790817499160767\n",
            "902 0.4977421462535858\n",
            "903 0.49757617712020874\n",
            "904 0.4974103271961212\n",
            "905 0.49724462628364563\n",
            "906 0.4970789849758148\n",
            "907 0.49691343307495117\n",
            "908 0.49674805998802185\n",
            "909 0.4965827465057373\n",
            "910 0.4964175522327423\n",
            "911 0.49625250697135925\n",
            "912 0.49608755111694336\n",
            "913 0.49592268466949463\n",
            "914 0.49575793743133545\n",
            "915 0.4955933094024658\n",
            "916 0.49542883038520813\n",
            "917 0.4952644407749176\n",
            "918 0.49510011076927185\n",
            "919 0.49493592977523804\n",
            "920 0.4947718381881714\n",
            "921 0.49460792541503906\n",
            "922 0.49444401264190674\n",
            "923 0.49428027868270874\n",
            "924 0.4941166937351227\n",
            "925 0.4939531087875366\n",
            "926 0.4937897324562073\n",
            "927 0.4936263859272003\n",
            "928 0.4934632480144501\n",
            "929 0.493300199508667\n",
            "930 0.4931372404098511\n",
            "931 0.49297428131103516\n",
            "932 0.49281153082847595\n",
            "933 0.4926489591598511\n",
            "934 0.4924863874912262\n",
            "935 0.49232396483421326\n",
            "936 0.4921616315841675\n",
            "937 0.49199944734573364\n",
            "938 0.4918373227119446\n",
            "939 0.49167537689208984\n",
            "940 0.4915134906768799\n",
            "941 0.4913517236709595\n",
            "942 0.4911900758743286\n",
            "943 0.49102848768234253\n",
            "944 0.490867018699646\n",
            "945 0.490705668926239\n",
            "946 0.49054446816444397\n",
            "947 0.4903833866119385\n",
            "948 0.490222305059433\n",
            "949 0.4900614321231842\n",
            "950 0.4899006187915802\n",
            "951 0.48973995447158813\n",
            "952 0.48957934975624084\n",
            "953 0.4894188642501831\n",
            "954 0.48925846815109253\n",
            "955 0.4890982508659363\n",
            "956 0.4889381229877472\n",
            "957 0.4887780547142029\n",
            "958 0.48861807584762573\n",
            "959 0.4884582459926605\n",
            "960 0.48829856514930725\n",
            "961 0.48813891410827637\n",
            "962 0.4879794418811798\n",
            "963 0.487820029258728\n",
            "964 0.4876607358455658\n",
            "965 0.48750150203704834\n",
            "966 0.4873424172401428\n",
            "967 0.48718351125717163\n",
            "968 0.4870246350765228\n",
            "969 0.4868658483028412\n",
            "970 0.4867071509361267\n",
            "971 0.48654860258102417\n",
            "972 0.48639023303985596\n",
            "973 0.48623180389404297\n",
            "974 0.4860735535621643\n",
            "975 0.4859154522418976\n",
            "976 0.485757440328598\n",
            "977 0.48559948801994324\n",
            "978 0.4854416847229004\n",
            "979 0.48528406023979187\n",
            "980 0.4851263761520386\n",
            "981 0.4849688708782196\n",
            "982 0.4848115146160126\n",
            "983 0.48465415835380554\n",
            "984 0.48449698090553284\n",
            "985 0.4843398928642273\n",
            "986 0.4841829836368561\n",
            "987 0.48402607440948486\n",
            "988 0.4838693141937256\n",
            "989 0.48371267318725586\n",
            "990 0.4835561215877533\n",
            "991 0.4833996295928955\n",
            "992 0.48324331641197205\n",
            "993 0.48308706283569336\n",
            "994 0.4829309582710266\n",
            "995 0.48277491331100464\n",
            "996 0.4826189875602722\n",
            "997 0.48246315121650696\n",
            "998 0.48230740427970886\n",
            "999 0.4821517765522003\n",
            "predict i hour 1.0 tensor(False)\n",
            "predict 7 hour 7.0 tensor(True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "id": "AFP_hHYrTr7t",
        "outputId": "93850fa8-4271-495e-c7be-11f62f99cb80"
      },
      "source": [
        "import numpy as np\n",
        "xy = np.loadtxt('data-diabetes.csv', delimiter =',', dtype = np.float32)\n",
        "x_data = Variable(torch.from_numpy(xy[:, 0:-1]))\n",
        "y_data = Variable(torch.from_numpy(xy[:,[-1]]))\n",
        "\n",
        "class Model(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super (Model, self).__init__()\n",
        "    self.l1 = torch.nn.Linear(8.6)\n",
        "    self.l2 = torch.nn.Linear(6.4)\n",
        "    self.l3 = torch.nn.Linear(4.1)\n",
        "\n",
        "    self.sigmoid = torch.nn.Sigmoid()\n",
        "\n",
        "  def forward(self, x):\n",
        "    out1 = self.sigmoid(self.l1(x))\n",
        "    out2 = self.sigmoid(self.l2(out1))\n",
        "    y_pred = self.sigmoid(self.l3(out2))\n",
        "    return y_pred\n",
        "\n",
        "model = Model()\n",
        "\n",
        "criterion = torch.nn.BCELoss(size_average=True)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "for epoch in range(100):\n",
        "  y_pred = model(x_data)\n",
        "\n",
        "  loss = criterion(y_pred, y_data)\n",
        "  print(epoch, loss.data[0])\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-dba440877217>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mxy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data-diabetes.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mx_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mloadtxt\u001b[0;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows)\u001b[0m\n\u001b[1;32m    959\u001b[0m             \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_string_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 961\u001b[0;31m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    962\u001b[0m             \u001b[0mfencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'encoding'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[1;32m    533\u001b[0m                                       encoding=encoding, newline=newline)\n\u001b[1;32m    534\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s not found.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: data-diabetes.csv not found."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "id": "nYUHT-2k0niY",
        "outputId": "6da5fe87-84b0-4f9b-8b60-9188909166b4"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class DiabetesDataset(Dataset):\n",
        "  def __init__(self):\n",
        "    xy=np.loadtxt('data-diabetes.csv', delimiter=',', dtype=np.float32)\n",
        "    self.len = xy.shape[0]\n",
        "    self.x_data = torch.from_numpy(xy[:, 0:-1])\n",
        "    self.y_data = torch.from_numpy(xy[:, [-1]])\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.x_data[index], self.y_data[index]\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.len\n",
        "\n",
        "dataset = DiabetesDataset()\n",
        "train_loader = DataLoader(dataset =dataset,\n",
        "                          batch_size =32,\n",
        "                          shuffle=True,\n",
        "                          num_workers=2)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out1 = self.sigmoid(self.l1(x))\n",
        "    out2 = self.sigmoid(self.l2(out1))\n",
        "    y_pred = self.sigmoid(self.l3(out2))\n",
        "    return y_pred\n",
        "\n",
        "model = Model()\n",
        "\n",
        "criterion = torch.nn.BCELoss(size_average=True)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "for epoch in range(2):\n",
        "  for i, data in enumerate (train_loader, 0):\n",
        "    inputs, labels = data\n",
        "    inputs, labels = Variable(inputs), Variable(labels)\n",
        "    y_pred = model(inputs)\n",
        "    loss = criterion(y_pred, labels)\n",
        "    print(epoch, i, loss.data[0])\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-bf058678551d>\"\u001b[0;36m, line \u001b[0;32m25\u001b[0m\n\u001b[0;31m    def forward(self, x):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    }
  ]
}